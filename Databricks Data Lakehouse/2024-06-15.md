## DataBricks lakehouse platform
- databricks is the inventor of data lakehouse architecture
- ideal data and AI pf for all data types with single security and governance model
- cloud agnostic
	- data can be governed wherever it is stored
- reliability and performance of delta lake as the data lake foundation
- fine grained governance for data and AI with unity catalog
- support for persona based use cases
- lh pf arch also provides instant serverless compute access
	- db provides and manages the compute layer on behalf of the customer
![[Pasted image 20240615111127.png]]![[Pasted image 20240615111157.png]]
- dlhpf eliminates challenges caused by previous data envs:
	- data silos
	- complicated structures
	- fractured governance and security structure
- dlhpf is:
	- simple
		- single pf for dwh and ai use cases
	- open
		- foundation on delta lake for reliability and performance
		- avoids proprietary sys, thus easy sharing of data
			- allows us to build the data ecosystem we need with open source projects and vast nw of db partners
		- built on open source and open stds
	- multi-cloud
		- one consistent data pf across clouds

## DLHPf Architecture and Security fundamentals
#### Importance of data reliability and performance on pf arch
- bad data in = bad data out
	- data used to build business insights must be reliable and clean
	- dl are great soln for holding large amount of raw data
		- lack features for data reliability and quality that often make them data swamps
	- dl often do not give better performances like dwhs
- problems encountered when using data lakes
	- lack of ACID txn support
		- impossible to mix updates, appends and reads
	- lack of schema enforcement
		- creating inconsistent low quality data
	- lack of integration with data catalog
		- dark data and no single source of truth
	- these 3 points raise a question on data reliability
	- performance:
		- using object storage means data is stored in immutable files
			- leading to ineffective partitioning and too many small files
			- partitioning is ineffective if the wrong column is chosen or due to high cardinality data
			- small file known query performance degradation issue
			- partitioning - poor man's indexing practice
				- much time wasted in tuning file sizes for performance
				- appending new data just means adding new files
- dlh solves data reliability and performance issues with two foundational technologies
	- delta lake 
	- photon
## delta lake
- file based open source storage format
- it provides ACID txn guarantees
	- so no partial corrupted files
- scalable data and metadata handling
	- leveraging spark to scale out all md processing
	- handling md for pb scale tables
- audit history and time travel
	- provides txn log
	- every change to data included, full audit trail
	- roll back of txns possible
- schema enforcement and schema evolution
	- denying inserts with the wrong schema
	- also allows table schema to be explicitly and safely changed
- supports for deletes, updates and merges
	- change data capture, scd and streaming upserts use cases are supported
- unified streaming and batch data processing
	- allowing data teams to work with wide variety of data latencies
	- even interactive queries
- delta lake runs on top of existing data lakes and is compatible with Apache Spark
- uses delta tables
	- based on parquet
	- common format for structuring data
	- delta tables works with semi and unstructured data
	- providing versioning, reliability, md management, time travel
- txn log
	- key to many of above features
	- single source of truth
	- when a user reads a delta lake table for the first time or queries an open table, 
		- spark checks the txn log for new txns
		- if a change exists, spark updates the table
	- prevents the user from making a divergent or conflicting change to a table
- is an open source project
![[Pasted image 20240615191519.png]]

## Photon
- arch of lakehouse can pose challenges with the underlying query executing engine for accessing and processing structured and unstructured data
- execution engine must be able to provide similar or better performance of DWH while having the scalability of DL
- soln to the processing challenges in the DLH arch: photon
- next gen query engine
	- provides dramatic infra cost savings
![[Pasted image 20240615192013.png]]
- photon is compatible with spark APIs
	- implementing more general exec fw for efficient processing with the support of SPARK API
	- we see increased speed for many use cases
- significant speed ups using photon
	- SQL based jobs
	- IOT use cases
	- loading data into delta and parquet
- photon is compatible with spark df and SQL apis
	- to allow wls to run without making code changes
- photon ***coordinates work and resources***, transparently accelerating portions of queries without tuning or intervention
- photon initially used to accelerate SQL use cases, now in scope for all data and analytical wls
- first purpose built lh engine for data performance