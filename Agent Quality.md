
50
14
- trajectory over the result
	- the path the agent chooses is the truth
	- quality checks are more about the trajectory than the results
	- when the agent chooses a rogue path but still ends up with the expected results, it is a quality issue
- to be able to view the trajectory
	- logging, tracing metrics to peer into agent's reasoning
- evaluation is a continuous loop
	- how the agent performs in the real world is also taken to improve the agent's quality
- agent's typical failure modes:
	- algorithmic bias
		- resume screening agent learning and amplifying biases from old hiring data
	- factual hallucination
		- agent making up sources and data
	- performance and concept drift
		- world changes and the agent does not keep up
	- emergent unintended behavior
		- agents develop weird superstition about what actions get rewarded 
		- finds clever loopholes in system rules in order to achieve goal
- how does the testing in regular programming and agents differ?
	- in traditional programming, we perform verification
		- we check if what we built matches the spec
		- did we build the product right?
	- whereas in agent evaluation, we perform validation
		- there is non-determinism involved as the agent's core logic is probabilistic and failure is not explicit crashes
		- did we build the right product
- 4 key pillars of quality
	- effectiveness: did the agent achieve what the user intended
		- customer service agent: not just about closing the agent but also whether the agent solved the complaint
	- efficiency: solved well, cheaply, quickly
		- complexity of the path
	- robustness: curveball handling
		- API errors, nw issues, unclear instructions from user
		- how does the agent handle
	- safety and alignment
		- dealing with prompt injection
		- sticking to constraints, ethics and avoiding harm
- how to judge the agent against these 5 pillars?
	- outside in hierarchy
	- start with the result and understand if the result satisfies the user
	- once failure is identified, we move to the inside out view
	- we analyze the agent's approach by systematically assessing every component of its execution strategy
	- ![[Pasted image 20251119123603.png]]
- what are the output evaluation metrics?
	- Task success Rate:
		- a binary (or graded) score of whether the final output was correct, complete and solved the user's problem
	- User Satisfaction
		- for interactive agents this can be direct user feedback(thumbs up and down) or a customer satisfaction score(CSAT)
	- Overall Quality
		- if the agent's goal was quantitative(summarize these 10 articles), the metric might be accuracy or completeness(ex: did it summarize all 10?)
- what are the components of the execution trajectory? 
	- LLM Planning - the thought: is the llm itself is the problem
	- Tool Usage - selection and parameterization
	- tool response interpretation
	- RAG performance
	- trajectory efficiency and robustness
	- multi-agent dynamics
- what are the common failures at the planning stage?
	- hallucinations, nonsensical or off-topic responses, context pollution or repetitive output loops
- what are the failures during tool usage?
	- calling of wrong tools, failing to call a necessary tool, hallucinating tool names or parameter names/types, or calling one unnecessarily
	- even if appropriate tool is selected, failure can be seen by providing missing parameters, incorrect data types, or malformed json for API call
- what are the failures during tool response interpretation?
	- the response being ignored for next course of action
	- wrong insights retrieved from the response
	- misinterpretation of data
	- not recognizing the error state returned by the tool(ex: API 404 error)
- what are the failures with RAG Performance?
	- retrieving irrelevant or outdated or incorrect data
	- ignoring the retrieved data and hallucinating an answer
- what evaluations are done while checking for trajectory efficiency and robustness?
	- the process: exposing inefficient resource allocation, such as an excessive number of API calls, high latency or redundant 
- what evaluation takes place with multi-agent dynamics?
	- trajectories involve multiple agents
	- check inter-agent communication logs to check for misunderstandings or communication loops and ensure agents are adhering to their defined roles without conflicting with others
- how do we perform evaluation covering all these components?
	- hybrid approach: automated systems provide scale, but human judgement remains crucial arbiter of quality
- how are automated metrics useful?
	- great place to start
	- useful from the aspect of reproducibility, surface level measurement and benchmarking outputs
	- they are not efficient in marking the reasoning or user value
- what are some of the automated metrics?
	- String-based similarity(ROGUE, BLEU): comparing generated text to references
	- embedding based similarity(BERTScore, cosine similarity): semantic closeness
	- Task-specific benchmarks, ex: TruthfulQA
- tip from the paper for regression testing
	- save trajectory and result of a successful turn for a prompt
	- after any changes, use the same prompt and test it out
	- if we run the test case later and if the agent deviates from the expected, we know something is broken
- after we use automated metrics, what is the next step? 
	- LLM as a judge
	- this scales the testing that can be done
- what goes on with LLM as a judge?
	- involves using a powerful, state of the art mode(like gemini advanced) to evaluate the outputs of another agent based on the input that was given
	- we provide judge LLM with the agent's output, the original prompt, the golden answer or reference and a detailed evaluation rubric(ex: Rate the helpfulness, correctness, and safety of this response on a scale of 1-5, explain your reasoning)
	- a better way to implement this method of testing is to give the same prompt to both current prod agent and the dev agent, pass the responses to a judge and tell it to pick a winner based on a rubric(depending upon the task at hand)
		- a high win rate is far more reliable signal of improvement than a small change in an absolute 1-5 score
- what lacks with LLM as a judge approach?
	- even LLM as a judge is at a surface level, as it only gives us a feeling of whether the response is helpful, clear and better
	- the trajectory is still not looked at
- what are solutions to overcome shortcomings of LLM as a judge approach?
	- agent as a judge
- what goes on with agent as a judge approach of testing?
	- a specialized agent is used to test
	- entire trajectory is looked at and assessed
	- key dimensions of evaluation include:
		- plan quality: was the plan logically structured and feasible? 
		- tool use: were the right tools chosen and applied correctly?
		- context handling: did the agent use prior information effectively? 
	- we must configure the agent framework to log and export the trace, including the internal plan, the list of tools chosen and the exact arguments passed
	- then, create a specialized critic agent with a prompt(rubric) that asks it to evaluate this trace object directly
	- the questions must be specific: was the initial plan logical, was the tool choice appropriate, were the arguments passed correctly 
- what are the shortcomings of agent as a judge?
	- lacks deep subjectivity and complex domain knowledge
- what's the next level solution for testing?
	- human in the loop evaluation
- what is Human in The Loop Evaluation not?
	- in certain areas, human evaluation is not objective ground truth
	- assessing creative quality or nuanced tone
- what is the use of HITLE?
	- ensuring agent's behavior aligns with complex human values, contextual needs, and domain-specific accuracy
- what are the key functions that are part of HITL? 
	- domain expertise: domain experts to evaluate factual correctness and adherence to specific industry standards
	- interpreting nuance: complex ethical alignment, tone, creativity, user intent
	- creating the golden set: which will act as a benchmark, involves curating comprehensive evaluation set, defining the objectives for success, and crafting a robust suite of testcases
	- for really important tasks, HITL becomes a security mechanism where the human approves or rejects an action
- how to make HITL efficient?
	- provide with a UI where one side the conversation history is displayed and on the other we see the agent's internal reasoning
- how is safety evaluated?
	- systematic red teaming: actively trying to break the agent using negative cases
		- this includes attempts to generate hate speech, reveal private info, propagate harmful stereotypes etc
	- automated filters & human review: implementing technical filters to catch policy violations and coupling them with human review
	- adherence to guidelines: explicitly evaluating the agent's outputs against predefined ethical guidelines and principles to ensure alignment and prevent unintended consequences
- how can we design guardrails?
	- ADK provides us capabilities to add guardrails as plugins(reusable modules)
	- the plugin we create would register it's methods with the framework's available ***callbacks***
	- ex: our plugin's check_input_safety() method would register with the before_model_callback
		- this method's job is to run a prompt injection classifier
	- ex: check_output_pii() method would register with the after_model_callback
		- checks for pii in the output
- what is the difference between monitoring and observability?
	- monitoring is like checking if the line chef follows a recipe to to the dot where we verify a known and predictable process
	- observability: understanding the process and reasoning and quality of these tasks
- what are the three pillars of observability?
	- logs, traces and metrics
- how are logs similar and different than the usual case?
	- timestamped entries in the agent's diary
	- they tell us what happened
	- in case of agents, we need logs that can help us reconstruct the agent's thought process
	- a structured json format is the gold standard
	- some components that should be captured are: prompt/response pairs, intermediate reasoning steps, structured tool calls and any changes to the agent's internal state
- what is tracing? 
	- traces stitch the individual logs(spans) together to help us under the cause effect relationship end-to-end
	- tracing follows single task - from initial query to the final result
	- A Trace reveals the full causal chain: User Query → RAG Search (failed) → Faulty Tool Call (received null input) → LLM Error (confused by bad tool output) → Incorrect Final Answer
- what are the key elements of an agent trace?
	- the modern tracing is build on standards like openTelemetry
	- spans: individual named operations within a trace
		- an llm_call span, a tool_execution span
	- attributes: rich metadata attached to each span - prompt_id, latency_ms, token_count, user_id
	- context propagation: magic link that links spans together via a unique trace_id
		- allows trace services to assemble the full picture
- what are metrics? 
	- quantitative, aggregated health scores that give us an immediate, at a glance understanding of our agent's overall performance
	- derived by aggregating the data from our logs and traces over time
	- they answer: "how well did the performance go, on average?"
	- metrics can be divided into two different categories: 
		- the directly measurable system metrics 
		- the more complex, evaluative quality metrics
- what are system metrics?
	- quantitative measures of operational health
	- purely calculated from the attributes on our logs and traces
	- applies different aggregation functions
- what are the key performance system metrics?
	- latency(p50/p99): median and 99th percentile response time calculated by aggregating the duration_ms attribute from the traces
	- error rate: % of traces that contain a span with an error = true
- what are the key cost system metrics? 
	- tokens per task: the average of the token_count attribute across all traces, which is vital for managing LLM costs
	- api cost per run: token_count * model pricing
- what are the effectiveness system metrics?
	- task completion rate: % of traces that successfully reach a designated success span
	- tool usage frequency: a count of how often each tool(ex: get_weather) appears as a span name, revealing which tools are most valuable
- what are quality metrics?
	- are second-order metrics derived by applying judgement frameworks
	- what are the judgement frameworks
	- to assess the agent's reasoning and final output quality itself
	- these are not simple counters or averages
	- this is where all the llm as a judge, agent as a judge and HITL come in - > judgement framework
- what are examples of quality metrics?
	- correctness & accuracy: did the agent provide a factually correct answer? if it summarized a content, was the summary true to it's content?
	- trajectory adherence: did the agent follow the intended path or "ideal recipe" for a given task? did it call the right tools in the right order?
	- safety & responsibility: did the agent's response avoid harmful, biased or inappropriate content?
	- helpfulness & relevance: was the agent's final response actually helpful to the user and relevant to their query
- capturing so much logs can become costly and add latency, what is the solution?
	- dynamic sample: trace 10 % of all successful requests but 100% of all failures
	- prod with lower higher level and dev with lower
- 52
- 1629
https://www.youtube.com/watch?v=LFQRy-Ci-lk