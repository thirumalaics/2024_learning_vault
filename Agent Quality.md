
50
14
- trajectory over the result
	- the path the agent chooses is the truth
	- quality checks are more about the trajectory than the results
	- when the agent chooses a rogue path but still ends up with the expected results, it is a quality issue
- to be able to view the trajectory
	- logging, tracing metrics to peer into agent's reasoning
- evaluation is a continuous loop
	- how the agent performs in the real world is also taken to improve the agent's quality
- agent's typical failure modes:
	- algorithmic bias
		- resume screening agent learning and amplifying biases from old hiring data
	- factual hallucination
		- agent making up sources and data
	- performance and concept drift
		- world changes and the agent does not keep up
	- emergent unintended behavior
		- agents develop weird superstition about what actions get rewarded 
		- finds clever loopholes in system rules in order to achieve goal
- how does the testing in regular programming and agents differ?
	- in traditional programming, we perform verification
		- we check if what we built matches the spec
		- did we build the product right?
	- whereas in agent evaluation, we perform validation
		- there is non-determinism involved as the agent's core logic is probabilistic and failure is not explicit crashes
		- did we build the right product
- 4 key pillars of quality
	- effectiveness: did the agent achieve what the user intended
		- customer service agent: not just about closing the agent but also whether the agent solved the complaint
	- efficiency: solved well, cheaply, quickly
		- complexity of the path
	- robustness: curveball handling
		- API errors, nw issues, unclear instructions from user
		- how does the agent handle
	- safety and alignment
		- dealing with prompt injection
		- sticking to constraints, ethics and avoiding harm
- how to judge the agent against these 5 pillars?
	- outside in hierarchy
	- start with the result and understand if the result satisfies the user
	- once failure is identified, we move to the inside out view
	- we analyze the agent's approach by systematically assessing every component of its execution strategy
	- ![[Pasted image 20251119123603.png]]
- what are the output evaluation metrics?
	- Task success Rate:
		- a binary (or graded) score of whether the final output was correct, complete and solved the user's problem
	- User Satisfaction
		- for interactive agents this can be direct user feedback(thumbs up and down) or a customer satisfaction score(CSAT)
	- Overall Quality
		- if the agent's goal was quantitative(summarize these 10 articles), the metric might be accuracy or completeness(ex: did it summarize all 10?)
- what are the components of the execution trajectory? 
	- LLM Planning - the thought: is the llm itself is the problem
	- Tool Usage - selection and parameterization
	- tool response interpretation
	- RAG performance
	- trajectory efficiency and robustness
	- multi-agent dynamics
- what are the common failures at the planning stage?
	- hallucinations, nonsensical or off-topic responses, context pollution or repetitive output loops
- what are the failures during tool usage?
	- calling of wrong tools, failing to call a necessary tool, hallucinating tool names or parameter names/types, or calling one unnecessarily
	- even if appropriate tool is selected, failure can be seen by providing missing parameters, incorrect data types, or malformed json for API call
- what are the failures during tool response interpretation?
	- the response being ignored for next course of action
	- wrong insights retrieved from the response
	- misinterpretation of data
	- not recognizing the error state returned by the tool(ex: API 404 error)
- what are the failures with RAG Performance?
	- retrieving irrelevant or outdated or incorrect data
	- ignoring the retrieved data and hallucinating an answer
- what evaluations are done while checking for trajectory efficiency and robustness?
	- the process: exposing inefficient resource allocation, such as an excessive number of API calls, high latency or redundant 
- what evaluation takes place with multi-agent dynamics?
	- trajectories involve multiple agents
	- check inter-agent communication logs to check for misunderstandings or communication loops and ensure agents are adhering to their defined roles without conflicting with others
- how do we perform evaluation covering all these components?
	- hybrid approach: automated systems provide scale, but human judgement remains crucial arbiter of quality
- how are automated metrics useful?
	- great place to start
	- useful from the aspect of reproducibility, surface level measurement and benchmarking outputs
	- they are not efficient in marking the reasoning or user value
- what are some of the automated metrics?
	- String-based similarity(ROGUE, BLEU): comparing generated text to references
	- embedding based similarity(BERTScore, cosine similarity): semantic closeness
	- Task-specific benchmarks, ex: TruthfulQA
- tip from the paper for regression testing
	- save trajectory and result of a successful turn for a prompt
	- after any changes, use the same prompt and test it out
	- if we run the test case later and if the agent deviates from the expected, we know something is broken
- after we use automated metrics, what is the next step? 
	- LLM as a judge
	- this scales the testing that can be done
- what goes on with LLM as a judge?
	- involves using a powerful, state of the art mode(like gemini advanced) to evaluate the outputs of another agent based on the input that was given
	- we provide judge LLM with the agent's output, the original prompt, the golden answer or reference and a detailed evaluation rubric(ex: Rate the helpfulness, correctness, and safety of this response on a scale of 1-5, explain your reasoning)
	- a better way to implement this method of testing is to give the same prompt to both current prod agent and the dev agent, pass the responses to a judge and tell it to pick a winner based on a rubric(depending upon the task at hand)
		- a high win rate is far more reliable signal of improvement than a small change in an absolute 1-5 score
- what lacks with LLM as a judge approach?
	- even LLM as a judge is at a surface level, as it only gives us a feeling of whether the response is helpful, clear and better
	- the trajectory is still not looked at
- what are solutions to overcome shortcomings of LLM as a judge approach?
	- agent as a judge
- what goes on with agent as a judge approach of testing?
	- a specialized agent is used to test
	- entire trajectory is looked at and assessed
	- key dimensions of evaluation include:
		- plan quality: was the plan logically structured and feasible? 
		- tool use: were the right tools chosen and applied correctly?
		- context handling: did the agent use prior information effectively? 
	- we must configure the agent framework to log and export the trace, including the internal plan, the list of tools chosen and the exact arguments passed
	- then, create a specialized critic agent with a prompt(rubric) that asks it to evaluate this trace object directly
	- the questions must be specific: was the initial plan logical, was the tool choice appropriate, were the arguments passed correctly 
- what are the shortcomings of agent as a judge?
	- lacks deep subjectivity and complex domain knowledge
- what's the next level solution for testing?
	- human in the loop evaluation
- what is Human in The Loop Evaluation  not?
	- in certain areas, human evaluation is not objective ground truth
	- assessing creative quality or nuanced tone
- what is the use of HITLE?
	- ensuring agent's behavior aligns with complex human values, contextual needs, and domain-specific accuracy
- 2046
https://www.youtube.com/watch?v=LFQRy-Ci-lk