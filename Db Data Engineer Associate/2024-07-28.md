## Autoloader
- what is autoloader?
	- auto loader provides highly efficient way to incrementally process new data
	- guarantees each file is processed exactly once
	- autoloader used for ***[file ingestion pipeline](https://docs.databricks.com/en/ingestion/auto-loader/index.html)***
	- auto loader provides a structured streaming source called cloudFiles
	- cloudFiles source automatically ***processes new files as they arrive***, with an option of also processing existing files in that directory
	- has support for python and SQL in DLT
	- use to process billions of files and near real time ingestion of millions of files per hour
	- autoloader can handle many formats
- how to process files the arrive incrementally without auto-loader?
	- let's assume that the files arrive at 3AM every day with data of the previous day
	- the files may arrive partitioned(within a date folder) or without any organization:
	- there are two potential ways to consume the new files:
		- when data is not organized, list the files in the bucket and read the ones with the most recent mod date time, but this can be intensive if the bucket has too many files
			- we need to scan the entire bucket each time
		- if the data arrives partitioned(folders) we can read just the folder of the last day
			- but what if the data arrives late?
				- with the current pattern, when we come to read tomorrow we will not be considering this late file of today
			- one solution can be to process previous X days files every day, so that we avoid missing any files but this requires extra compute
			- another solution can be a hybrid approach, we list the X days file and read files that arrived post 3 AM on other days and process all files of today
				- this solution may face the same problem as the first solution
- how does auto-loader solve the problems of non-autoloader solutions?
	- we point to the root directory which contains the partitioned folders/unorganized files
		- so as to make sure that all files are captured irrespective of which partition folder they arrive in or when
	- Autoloader sets up a queue with all files in the directory, these will be considered for the processing the first time the job runs
		- it is going to process everything in the queue and write out the status to two md folders
		-  commits and offsets
			- commits folder helps to keep progress of the data and files that have been loaded
				- makes recovery from failure painless
			- the offsets folder tells where autoloader should start reading from on the next run
https://medium.com/@leighrobertson512/why-you-should-use-databricks-autoloader-252f75e9e47d