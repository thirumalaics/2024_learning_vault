## Orchestrating jobs
- db allows us to schedule one or multiple tasks as part of a job
- here job consists of different tasks and these tasks in the course included:
	- Land_New_Data
	- DLT
	- pipeline results
- i think each task requires execution of a notebook
- these multi-task jobs are created from the workflows tab on db
	- workflows menu allows us to create jobs
- each task must be associated with a
	- task name
	- cluster
		- we can either allocate existing cluster or new cluster for the task
		- there is something called the job cluster, he mentioned that in prod we must use this for cost savings
		- multiple tasks, at least within the same job, can share a cluster
- each task can have 
	- a notebook, py script, dtl pipeline etc...
		- the notebook can be located in our databricks workspace or git repository
		- if we are choosing DLT pipeline, we can choose the pipeline we created earlier
			- the DLTP runs in the cluster in which we created this pipeline
	- parameters to be passed
- when we start creating multiple tasks within the pipeline, we can add a field called Depends on and provide the tasks that the new task is dependent on
- there are two schedule trigger type:
	- None and Scheduled
- we can set email notifications, so we can be notified on job success, start and failure
- the job also comes with user and group access management
	- controls who can own, manage, view and run the job
- the UI seen here is similar to Dataflow in GCP
- each run of our job has it's own visualization, output and md
	- clicking on the tasks of a run provides us with output of the task
		- for notebooks, I saw cell level output
	- DLT pipelines scheduled as tasks do not produce results directly on the runs UI
		- provides a hyper link to view visualization of a DLT pipeline
- runs which have failed tasks comes with a repair run button
	- allows us to run only the failed tasks

## DB SQL
- so far we have been working with data science and engineering workspace
- to work with DBSQL, we need to switch to a SQL persona
- DBSQL is a DWH that allows us to run all our SQL and BI applications at scale and with a unified governance model
- a few menus in the DBSQL persona:
	- SQL Editor
	- Queries 
	- Dashboards
	- Alerts
	- Data explorer
	- SQL WHs
		- we can create a SQL WH
		- SQLWH is the compute power of DB SQL
		- it is a SQL engine or endpoint based on a Spark Cluster
		- Create SQL WH helps us configure a new SQL engine
			- it creates a cluster where we can control configs like:
				- cluster size
				- auto stop interval
				- scaling
				- etc
			- we are also prompted to set permissions
- Dashboard creates dashboards
	- we are allowed to refresh the DSB
		- to re run queries behind each graph and refresh the data
		- queries behind the graphs can be easily seen with a few clicks
		- in the query the table is referenced similar to bq:
			-  \`samples\`.\`nyctaxi\`.\`trips\`
			-  \`catalog\`.\`db\`.\`table\`
	- visualizations can be edited
		- setting x, y columns
		- setting group by column
	- multiple visualizations can be added for a single query result
	- visualizations are different from dashboards
		- dashboards are collections of visualizations
	- visualizations can be added to dashboards
	- we can share our dashboard with other users and allow them to run the query for the visualization
	- only users with access to underlying data can run the dashboard queries even if they have can run access on the dashboard
	- while sharing a dashboard we can allow the user to run the dashboard using 
		- owner crds
		- viewer creds
	- we will have to be connected SQL WH to be able to run queries
		- may be corrected later
		- choose the catalog, by clicks, to query
			- then choose database
			- very very similar to BQ