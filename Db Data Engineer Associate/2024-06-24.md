## Delta lake
- open source storage fw that brings reliability to data lakes
- data lakes have
	- data inconsistencies and performance issues
- delta lake is
	- open-source tech
	- storage fw/layer and not storage format/medium
	- enables building dlhs and not dwh/database service
![[Pasted image 20240624090631.png]]
- delta lake is a component that is deployed on the cluster as part of db run time
- if we are creating delta lake table, it gets stored on the storage as one or more parquets
	- additionally, there is a txn log as well called delta lake txn log aka delta log
- delta log is ordered records of every txn performed on tables since their creation
- single source of truth
- every time we query a table, spark checks the txn log to retrieve the most recent version of the data
- each committed txn is recorded in a JSON file
	- each txn contains the operation that has been performed(insert/update) and predicates used during the operation
		- predicates ~ conditions and filters
	- all data files affected(added/removed) because of this operations is also part of the log
![[Pasted image 20240624091615.png]]


- in the following case, the writer process wants to update a record in file 1
- updates do not happen in place
	- a copy of file 1 is made and the update will be in that file
	- a new json log file is written where file 1 is invalidated
- when the reader process reads the most recent txn log, it tells that only 2 and 3 files are part of the current table version, hence reader reads only those
	- how does the reader process know which is the most recent txn log?
![[Pasted image 20240624091724.png]]
- important point is that, the txn log json gets written only after the write/change operation ***has completed successfully***
	- in the following, the write starts and there is a read action performed b4 the completion of write
	- in this case the most recent txn log is 001.json which only points to 2 and 3 files as the full version of the table
	- these files are read and after the write completion, txn log 002.json is added
	- most recent version of the data is guaranteed
	- read operation will never have a deadlock state or conflicts with any ongoing operation on the table

![[Pasted image 20240624092143.png]]
- failed file writes do not get a log entry, so for the reader process that comes in, it is as if there was no attempt at a file write and it will read only the 2,3,4
![[Pasted image 20240624092540.png]]

- Delta Lake Advs
	- ACID txns to object store
	- handle scalable md
	- full audit trail of all changes
	- builds upon std data formats

- delta lake is the default format for tables, no need to use USING DELTA keywords
- my cluster had terminated but I was not able to restart it either from nb or from compute
	- deleted the cluster and created a new one
- i have catalog tab instead of data tab where I am able to see the tables that I have created
![[Pasted image 20240624094120.png]]
- since no database was mentioned in the create statement, the table got created in default database
![[Pasted image 20240624094224.png]]
- DESCRIBE DETAIL describes md of our table, there are many columns displayed
	- as mentioned below, the SQL cell result is stored in a df with var name: \_sqldf
	- I was able to use this df as any other df
	- one ex col: location(path in dbfs where the parquets reside)
![[Pasted image 20240624094346.png]]

- \_delta_log is part of the folder where our logs reside, which is the value in col location
![[Pasted image 20240624094734.png]]

![[Pasted image 20240624094843.png]]

