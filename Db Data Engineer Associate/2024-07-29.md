- how is autoloader [different]([What is Auto Loader? | Databricks on AWS](https://docs.databricks.com/en/ingestion/auto-loader/index.html#benefits-of-auto-loader-over-using-structured-streaming-directly-on-files)) from structured streaming?
	- even though we can read file sources using spark structured streaming, auto loader comes with different benefits
	- ***scalability***: 
		- discovers billions of files efficiently. backfills can be performed asynchronously to avoid wasting any compute resources
	- ***performance***: 
		- the cost of discovering files with autoloader scales with num of files ingested instead of num of directories that the files may land in
	- ***Schema inference and evolution support***: 
		- auto loader can detect schema drifts, notifies us when schema changes happen and rescue data that would have been otherwise ignored or lost
	- ***cost***: 
		- AL uses cloud native apis to get list of files that exist in storage. even notification based file discovery is possible
		- setting up of this notification service is automatic and much cheaper than listing directories
- how does autoloader discover new files?
	- 2 ways
		- [file notification mode]([What is Auto Loader file notification mode? | Databricks on AWS](https://docs.databricks.com/en/ingestion/auto-loader/file-notification-mode.html)): check for arrivals via logs
			- autoloader automatically sets up notification service and queue service that subscribe to file events for the directory
			- scalable than the below mode
		- [directory listing mode]([What is Auto Loader directory listing mode? | Databricks on AWS](https://docs.databricks.com/en/ingestion/auto-loader/directory-listing-mode.html)): default
			- autoloader lists the input directory and considers only the files that were not processed earlier
			- allows to quickly start auto loader streams without any permission configs other than access to our data

## Delta Live Tables
- what are delta live tables?
	- soln to build, manage reliable and robust data engineering pipelines
		- to load streaming and batch data
		- using a declarative approach
	- simplifies the ETL dev process
	- provides reliable automatic data testing capabilities
	- provides monitoring and recovery of data
	- DLT responsible for performing data transformation, orchestrating tasks, cluster management, monitoring, data quality and error handling
- what are the benefits of DLT?
	- accelerates ETL process by providing declarative approach
		- takes care of orchestrating DAG, handle retries, changing data - wtf?
		- normal spark pipelines also get it done right?
	- managed infra - takes care of complex activities like: recovery, auto-scaling, and performance optimization against the workflow
	- ensure high data quality - provides quality control, testing, monitoring, and enforcement
	- unify batch and streaming