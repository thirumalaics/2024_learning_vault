- will the query that queries the output table of streaming write be a streaming query?
	- no, it will be a normal query
- when running a query with available now trigger mode, does the query run continuously? 
	- the query does not run continuously
	- as all the available data is processed at one time across microbatches
```
spark.table('cust_count')
.writeStream
.trigger(availableNow=True)
.outputMode("complete")
.option("checkPointLocation","dbfs:/use/hive/warehouse/xyas/")
.table("final_results")
```

- weirdly, I was ***not able*** to see the results of the above table when I did a select \*, even though the streaming write ended in some time
	- but when I did a select \* with a limit clause, the data was displayed
	- it is an issue seen in the [community edition](https://www.reddit.com/r/databricks/comments/1e9wdbu/issue_with_dbfs_community_edition/) of DB


## Incremental Data Ingestion from Files
- what is incremental data ingestion from file?
	- ability to load data from new files that have been encountered since the last ingestion
	- we dont need to reprocess old files
- what are the mechanisms provided by DB to perform incremental data ingestion from files?
	- db provides two mechanisms for incremental processing of new files as they arrive
		- COPY INTO
		- Auto Loader
	- all the methods below load the new files into tables
- what is COPY INTO?
	- sql command
	- used to load data from a file location into a delta table
	- loads data idempotently and incrementally
- how does COPY INTO works?
	- ***each time this command is run***, it only copies the new files from the source location
		- from this I can deduce that the query does not continuously run
	- files that are loaded already are skipped
###### What is the syntax of COPY INTO command?
```
COPY INTO my_table
FROM '/path/to/files'
FILEFORMAT = <format>
FORMAT_OPTIONS (<format options>)
COPY_OPTIONS (<copy options>); 
```
###### provide one example of COPY INTO command?
```
COPY INTO my_table
FROM '/path/to/files'
FILEFORMAT=CSV
FORMAT_OPTIONS ('delimiter' = '|',
				'header' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true') # specifying that the schema can be evolved
```

- What is Auto Loader?
	- uses structured streaming to process new data files as they arrive in the storage location
	- can be used to load billions of data files to the table
	- can support real time ingestion of millions of files per hour
	- uses checkpointing to track the ingestion process and to store metadata of the discovered files
	- exactly once and resumability guarantees
###### How to use auto loader in pyspark?
```
spark.readStream
.format("cloudFiles")
.option("cloudFiles.format",<source_format>)
.option("cloudFiles.schemaLocation", <schema_directory>)
.load('/path/to/files')
.writeStream.option('checkpointLocation', <checkpoint_directory>)
.option("mergeSchema", "true")
.table (<table_name>)
```
- in the above location, auto loader will detect new files as they arrive and ***queue them*** for ingestion
- uses structured streaming
- the query keeps running looking for new files
- auto loader can infer schema of our data
- it can detect any update to the ***fields*** of the source data set
- repetitive inference of schema is costly
	- to avoid the cost of inference each and every time during the stream, it helps to store the inferred schema to a location to be used later
	- schemaLocation option above for the cloudFiles is the location where auto loader can store the schema inferred


###### when to use auto loader or copy into?
- important points to consider while choosing:
	- quantity of files:
		- if in millions, opt for auto loader
		- if in thousands, opt for copy into
		- why?
			- COPY INTO less efficient at scale
			- auto loader can split the processing into multiple batches so it is more efficient at scale
		- db recommends to ***use auto loader*** when we are ingesting data from ***cloud object storage***

- i was able to display results written by autoloader by select * the output table with a limit clause
