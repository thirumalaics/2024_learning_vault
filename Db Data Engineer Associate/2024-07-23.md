- guarantees provided by spark structured streaming
	- fault-tolerance
		- resumability
	- exactly-once guarantee
		- idempotent sinks
		- multiple writes of the same data do not result in duplicates
- the above two guarantees work only if the ***stream source is repeatable***
	- this allows to ensure end-to-end exactly once semantics under any failure condition
- now he says, some operations are not supported by streaming dataframes
	- ex: sorting and dedup
	- too complex to implement or logically not possible
	- windowing and watermarking will help to do such operations over a subset of data
- I read parquet files in a dataframe
	- copy pasted an existing parquet file with new name to simulate new data file
	- the dataframe defined above did not reflect the new data
	- I defined a new dataframe on the same location and it did pickup all the files present
```
spark.readStream.table('orders').createOrReplaceTempView('first_sdf')
```
- when I try to query this temporary view, the query did not display results but in the video it showed results
	- the query will continue to run so that it can pick up new data
- plain ***order by*** or ***sorting*** are not supported, they simply error out
- views created on top of streaming dataframes will also be streaming temp views
`spark.table('streaming_temp_View')`
- the above statement returns a streaming dataframe as the view is streaming view
	- the above syntax returns a table/view as a dataframe
		- if the table/view is streaming, then the returned df is streaming and vice versa
- to support ***incremental writing***, incremental processing should be defined at the reading stage itself
```
spark.table('streaming_view')
.writeStream
.trigger(processingTime = '4 seconds')
.outputMode("complete")
.option("checkPointLocation", 'dbfs:/xyz')
.table('author_counts')
```

- above results are written in to a table
- for agg streaming query, output mode should be `complete` to overwrite table
- interactive dashboard is provided for every streaming query run in db
	- processing statistics displayed
- while the streaming write was continuing to happen, this guy was able to run sql queries in next cell
	- for some reason my streaming queries are not displaying any results like in the video
	- even the result of write operation is not viewable when I try to query the target table
		- even after I abort the stream write I am not able to see the results of the stream write in the table
		- when I described the table, the schema was as expected?
- when a streaming cell is running, we are allowed to run other cells irrespective of it has a query or pythonic code
	- but when a normal pythonic cell is running, we are not allowed to run similar pythonic cells at the same time
```
for i in range(1,20):
    print(i)
    time.sleep(10)
```
- tried running the above in two cells parallely
	- only the first triggered cell displayed the output while the second triggered cell literally said ***waiting*** for the first to complete