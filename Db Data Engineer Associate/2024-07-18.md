```
CREATE TABLE orders as SELECT * From PARQUET.`${dataset.bookstore}/orders`
```
- the above statement tries to create a folder for the table in `/user/hive/warehouse/` just like any other managed table creation
![[Pasted image 20240718090922.png]]
- even though the table got replaced with a different schema, the history tracks it as part of the original table
	- so we can go back in time before the schema change
![[Pasted image 20240718091032.png]]
- overwrite with a new column added
- now that the table has a new column, and if I try to use INSERT OVERWRITE it fails as the parquet has less number of columns
	- proof that INSERT OVERWRITE cannot be used to change the schema during overwrite
	- if this was CRAS, the schema would have been replaced and operation would have been successful, assuming no issues with file or infra
![[Pasted image 20240718091455.png]]

![[Pasted image 20240718091955.png]]

- column names do not matter when using INSERT OVERWRITE statement as long as the data types of the columns remain the same
- ![[Pasted image 20240718092136.png]]
- INSERT OVERWRITE treated as a WRITE operation in the history
- INSERT INTO statement does not have any built in guarantees to prevent inserting the same record multiple times
	- way of appending
	- as there is no built in guarantee of inserting only new records, causes duplicates
- MERGE INTO is the solution for avoiding duplicate insertions
	- upsert data from a view, table and dataframe into the target table
	- insert, update and delete
```
MERGE INTO customers c 
using customer_updates u
ON c.customer_id = u.customer_id
WHEN MATCHED AND c.email is null and u.email is NOT NULL THEN 
UPDATE SET email = u.email,
updated = u.updated
WHEN NOT MATCHED THEN INSERT *
```
![[Pasted image 20240718094551.png]]