- one common pattern that I noticed is that he reads tables(bronze, silver) using readStream, creates temp views on top of df and applies transformation by sourcing the temp view
	- the result of this transformation is stored in a temp view and this new temp view is used to write to the table of next layer
	- why read tables and create temp views on top of tables when one can query them directly?
		- an important goal in the video was to setup a stream that flows from source to bronze to silver to gold
		- to prove that reading from table is redundant, I did not read the table and create a temp view on top of it
		- I used the bronze table directly in the temp view to create the data for silver table
		- I tried reading the view that will produce data for the silver table using `spark.table` and tried write stream in order to stream write the view into table
			- i got the following error: `[[WRITE_STREAM_NOT_ALLOWED](https://docs.databricks.com/error-messages/error-classes.html#write_stream_not_allowed)] `writeStream` can be called only on streaming Dataset/DataFrame.`
			- the spark.table returned a static df and not a streaming df because table is treated as a static object even though data might be streamed into it
		- what went wrong:
			- I used the table in the temp view to create data for silver
			- the table is considered static
			- so I am forced to read the table as a stream and create a temp view on top of it, making it streaming view
			- then I can create the silver data view on top of this view which makes the silver view a streaming view
			- this allows me to read the streaming view using spark.table(as a streaming dataframe) and apply write stream on top of it
- the gold level of data in the video was aggregated and the write mode to this table was overwrite(complete)
	- this has implications
- what are the [limitations](https://docs.databricks.com/en/structured-streaming/delta-lake.html#id5) of structured streaming while reading?
	- Structured Streaming does not handle input that is not an append
		- so any update, delete, overwrite on the input is not accepted
		- by not accepted, I mean exception is thrown 
		- these type of changes ***cannot be automatically propagated downstream***
- are there any workarounds to by pass the above limitations? 
	- delete the output, checkpoint and restart the stream from beginning
		- I do not  fully understand this point
	- we can use either of the following options:
		- `ignoreDeletes`: ignore txns that delete data at partition boundaries
			- what are partition boundaries?
		- `skipChangeCommits`: ignore txns that delete or modify existing records
			- `skipChangeCommits` subsumes `ignoreDeletes`
			- `skipChangeCommits` deprecates `ignoreChanges` in the most recent version of DB runtime
			- the working of `skipChangeCommits` is different from `ignoreChanges`
			- `ignoreChanges` introduces duplicates
 


- to get all active queries associated with current session:
	- `spark.streams.active` - list of streaming query objects
```
for s in spark.streams.active:
    print("Stopping stream: " + s.id)
    # s.stop()
    # s.awaitTermination()
    print(s.name)
```