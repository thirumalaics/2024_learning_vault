## What is databricks?
- ***multi-cloud*** ***lakehouse pf*** based on ***apache Spark***
- lakehouse: ***unified analytics pf*** that ***combines*** best elements of ***data lakes and Dwhs***
	- Data Lake
		- open
		- flexible
		- ML Support
	- DWH
		- Reliable
		- Strong Governance
		- Performance
- DB uses the infra of the cloud providers to provision virtual machines or nodes of a cluster
	- this cluster comes with Databricks run time pre-installed
- DBLHPf is split into three layers
	- cloud service
		- aws, azure, GCP
	- runtime
		- Spark
		- Delta Lake
		- other sys libs
	- workspace
		- allows us to interactively implement and run de, data analytics and AI wls
![[Pasted image 20240617192948.png]]
## DBLHPf Arch
- how DB resources are deployed in our cloud provider?

![[Pasted image 20240617192925.png]]
- two high level components:
	- control plane and data plane
	- control plane resides within our DB account
	- data plane is in our cloud subscription
- whenever we create a db workspace, it is deployed in the control plane
	- deployment comes with web UI, Cluster Manager, Workflow Service and notebooks
- a storage account is deployed in the data plane in our subscription of cloud
	- it is used for databricks file sys or DBfs
	- for any spark cluster deployment, the cluster virtual machine will be deployed as part of data plane
- db will provide with the tools we need in order to use and control our infra
- db supports all the languages supported by Spark
	- scala, py, sql, r, Java
- batch and stream processing supported
- structured, semi and unstructured data can be processed

### DBfs
- db offers native support of a Distributed file system or DataBricksfilesystem
- any cluster created in db comes with DBfs
- DBfs is a abstraction layer, while it uses the underlying cloud storage to persist the data
	- ex: if we create a file in our cluster, and store it in DBfs, it is ***persisted in the underlying cloud storage***
		- ex: azure storage or s3 bucket
	- so data does not perish after cluster is deleted

- In the db workspace UI following tabs can be found, these are common tabs for all personas`
	- Repos - for git integration
	- data - to manage the dbs and tables
	- compute - to create and manage clusters
	- workflows - to deploy and orchestrate the jobs
- different interfaces for different users:
	- data science & engineering
	- ml enginers
	- data analysts