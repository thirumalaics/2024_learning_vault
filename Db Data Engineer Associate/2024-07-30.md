## Medallion Lakehouse Architecture
- multi-hop architecture
- what is medallion lakehouse architecture?
	- consists of multiple data ingestion layers and data goes through these layers before it is made available to the downstream team
- what are the layers in MLA?
	- bronze layer: data stored in original format
		- used for audit purpose and to track back to source data
			- for now, I can say data audit is process done to understand how and where the data is used in the organization
				- similar to the auditing that happens with money, we track where we spent and how much is spent
				- also where did the data/money came from?
			- i think it can also involve checking how the data came into picture
			- helps us understand our data better and allows us to make better decisions to leverage data
	- silver layer: responsible for cleaning and filtering bronze data
		- handle missing data, data type conversion
		- make data easy for querying
		- column renaming
	- gold layer: this layer contains the transformed, aggregated and modeled data which can be made available for sql queries
		- business centric and dimensional


## DLT - course
- what does DLT UI display?
	- DLT UI displays a DAG of tables
		- clicking on each table provides more information on that table
	- detailed logs are categorized based on the severity
	- Data quality metrics are displayed where we get percentage of dropped and written records for each table
		- the dropping and writing is decided based on the constraint we set during the pipeline definition
	- for every table, it shows metastore information
		- gives out fully qualified table name
		- the target dataset that we gave during pp creation is the dataset of this table
- can we modify the notebook code, where the tables are defined, after having visualized the pipeline?
	- yes, atleast in dev env of db
	- based on the change graph is modified automatically
	- it is important to remember, we should use `live.` identifier for dlt tables
- what is present in the pipeline storage directory that is given during the pipeline creation?
	- when he listed the location that he gave, there were 4 dirs:
		- autoloader, checkpoints, system, tables
		- system dir contains all events associated with the pipeline
			- these event logs are stored as delta table
			- we can query a delta table like this: ```SELECT * FROM delta.`path to events` ```
			- all the events we see in the ui are stored in this delta table
		- tables dir contains the dlt tables defined in the notebook

## CDC
- what is CDC ? 
	- change data capture: refers to the process of identifying the changes made to the data in data source, capturing and delivering the change to the target
		- change means: inserts, updates and deletes
- how do we identify changes?
	- changes are logged at the source as events that contain both the data of the records along with md info
		- this md says whether the record was inserted, updated or deleted
		- ![[Pasted image 20240730095356.png]]
		- also includes fields like timestamp or version number to identify the order of operations
		- the above picture depicts before the change is applied to target
- how does DLT support CDC processing?
	- using Apply changes into command
	- ```APPLY CHANGES INTO LIVE.target_table FROM STREAM(LIVE.cdc_feed_table) KEYS(key_field) APPLY AS DELETE WHEN operation_field="DELETE" SEQUENCE BY sequence_field COLUMNS *```
		- key field denotes the PK fields
			- if the key exists in the target table, the record will be updated
			- if not inserted
		- DELETE WHEN followed by the predicate determines when records must be deleted
