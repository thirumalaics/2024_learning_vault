## Advanced Delta lake features contd
- optimize command
	- helps us compacting small files and can also be extended to apply indexing
		- after compaction the old small files are not deleted
	- `optimize my_table zorder by col;`
		- notice that there is no `table` keyword
	- ![[Pasted image 20240626093643.png]]
	- what size file is considered as small file and when is a file considered not to be a small file?
	- the statement above as well provides a table output
	- optimize command is logged into the table version history, any `zOrder by` clause we give is logged as operation parameters
	- as seen in the above scrnsht, it says 5 files were removed but when I checked the location of the table in dbfs there are 7 files(remember there was an update as well), so the files are not literally removed
	- the 5 data files of the previous version are still there but they do not contribute to the current version of table
	- ![[Pasted image 20240626093818.png]]
	- ![[Pasted image 20240626094340.png]]

- vacuum command
	- when I ran the `VACUUM employees`, no files were deleted as the retention period is by default 7 days, if there are orphan files or files that do not contribute to ***recent version*** - they would have been deleted
	- vacuum does not delete files as per version, it purely goes by the retention period given
		- so if the last version is more than 7 days old and the retention period is given as 7 days, only the files contributing to the current version are retained
			- after the operation we cannot travel back from current version
	- if a file from the previous version that is not used in the current version exceeds the retention period, it is deleted
		- even though it is from 1 version below
	- ![[Pasted image 20240626094728.png]]
	- ![[Pasted image 20240626094741.png]]
	- the above vacuum command did not get an entry on the history table
	- if we give retention period to 0 hours, an error is thrown, zero hours will retain only the current version
		- ![[Pasted image 20240626095250.png]]
		- a minimum of 7 days should be the retention period's argument value
		- if we want to go below that, we need to alter a db config:
			- `SET spark.databrics.delta.retentionDurationCheck.enabled = false;`, run in a cell with sql language
			- disables the check on the retention period given
		- after this if we run the vacuum, it works as expected
		- ![[Pasted image 20240626095644.png]]
		- ![[Pasted image 20240626095756.png]]
		- the vacuum command does not wipe out version history table
			- that means, it does not wipe out contents of \_delta\_log
		- only the failed write files and the files that do not contribute to the recent version of table within the retention period are deleted
		- any attempt at querying an old version in this case is met with an error
		- ![[Pasted image 20240626100228.png]]
	- drop table command completely wipes out the table along with the delta log and data folders and files
		- we can no longer 
			- reference this table
			- see the history of the table
- file pattern matching was not possible with dbutils.fs
