## Notebook fundamentals
- while creating a notebook, we will have to mention which cluster we need to run it on
	- can be changed later
	- we can also First create the notebook and as part of it create cluster
- also specify language - can be changed any time
- notebooks to interactively develop and code on databricks
- collaboration is possible by sharing the notebook
- basic fnality of the notebook same as Jupyter notebook
	- with additional fnality
- notebook name can always be changed
- important to configure a notebook to a cluster
- created clusters which are timed out are still available to be restarted in order to use
	- they are not completely wiped out from the databricks interface, atleast as a configuration it is visible
	- in the lesson, his cluster had terminated but he was able to attach that cluster to a notebook
		- after attaching he restarted the cluster
- the time it takes for a cluster to start depends on many things
	- size of the cluster
	- diff configs like additional package or libraries installation
- in the notebook, the language is configurable on two levels:
	- notebook
	- cell
- changing the language on the cell level(diff lang than the nb level)
	- will add a magic command starting with \% followed by the lang name
	- magic commands are builtin commands that provide the same output regardless of the nb lang
![[Pasted image 20240621084152.png]]
- %scala is called as language magic command
	- it allows execution of code in a language other than the nb's default
- adding markdown titles adds the title name to the table of contents, which allows us to easily navigate the notebook
![[Pasted image 20240621084608.png]]
- the %run magic command allows us to run a different nb from the current nb
	- the notebook to be run can be referenced via absolute or relative path
	- if the path has components that have spaces in it, ***the entire path*** should be enclosed in double quotes
		- not single quotes, single quotes did not work
	- when we run a notebook from another notebook, it works more like import
		- at the very least the (variables and modules imported) in the nb that is run is imported into the current nb
![[Pasted image 20240621090009.png]]
- variables and other declarations from the "another one" nb is available in the current nb
- db also provides %fs magic commands to deal with file system operations
	- ![[Pasted image 20240621090327.png]]
	- the fs is very different from the system where the nbs reside
		- they are not the same file system
- another way to deal with fs operations is to use databricks utilities aka dbutils
	- provides a number of utility commands for configuring and interacting with the env
		- dbutils is a common utility handle that includes methods to interacts with more than just the file system
![[Pasted image 20240621090822.png]]
![[Pasted image 20240621090903.png]]
- dbutils more useful than the fs magic command
![[Pasted image 20240621091024.png]]
- db provides a spark based function called display that can parse the given input and display them as tables, limited to 1000 rows
	- ![[Pasted image 20240621091224.png]]
	- i passed in a list of strings, it failed
	- i passed in a list of list of strings, it parsed it as a df
	- ![[Pasted image 20240621091459.png]]
	- display fn's op can be exported/downloaded as only csv or be added to the nb dashboard
	- ![[Pasted image 20240621091554.png]]
- nb and folders are downloadable
	- folders can be downloaded as dbcloud archive which is nothing but a zip file that contains a collection of directories and notebooks
	- can be used to move or share nbs across db workspaces
	- we can import dbc archive into any ws easily
- change history in the 
![[Pasted image 20240621092031.png]]