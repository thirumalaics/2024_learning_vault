- when working with CSV files as our source, it is important that the order of columns must remain the same
	- why?
		- spark will load the data, apply column names and data types in the order specified during table creation
		- is this true for all data formats? #doubt 
			- [parquet](https://stackoverflow.com/questions/67607742/loading-parquet-files-with-different-column-ordering)
- using USING, is it mandatory to provide schema?
	- when I tried without giving schema, all columns were taken as string, tried for CSV
	- so no, it is not mandatory
- I think external delta tables can be created via the location syntax only if the given location is empty or host(either hosted or hosts) parquet files along with delta log folder
	- no other format supported for external delta tables
	- ![[Pasted image 20240714095840.png]]
	- this is an error that I got when trying to create a table using LOCATION key word to a folder hosting json files only without delta log folder
	- value given to the location key word cannot have back ticks, it was not accepting it without quotes
		- values given to location key word must be enclosed with quotes and not backticks
	- external tables are also considered delta tables
		- this is confirmed by the provider key, whose value will be delta
	- ![[Pasted image 20240714100602.png]]
	- the above is an output of `DESCRIBE EXTENDED table_name;`
	- when the above statement is run for table created using USING, below table is the output:
		- for the current example, CSV was used as the format
		- extra rows are printed as output
		- the options we gave during table creation are stored as storage properties
		- ![[Pasted image 20240714100922.png]]
		- the below example was created: `USING PARQUET`
			- ![[Pasted image 20240714101141.png]]
		- even if we create a table using USING on a location where a ext delta table is hosted, the `USING` table is not considered to be a delta table
			- when I tried querying the `USING` table, I was not able to
			- the error was
			- ![[Pasted image 20240714101838.png]]
			- same error when I created an USING table on top of a location which had a managed table
- interesting feature noticed:
	- run a select query to an external provider table
		- the result of this query is cached
	- write more data to the location of external provider table with the same format
		- now new data has come into this location
	- when the same select query is run the results do not show the new data
		- as subsequent queries are serviced from cache
	- external CSV provider is not configured to tell spark to reread
		- we can manually refresh the cache of our data by running `REfRESH TABLE table_name;`
		- this will invalidate the cache and require rescanning and recaching