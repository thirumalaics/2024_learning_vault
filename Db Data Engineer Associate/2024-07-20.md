- EXCEPT is done using MINUS at least within databricks
- pivot clause supported
## Higher Order functions and SQL UDfs
- to work on ***complex data types***, we need to use higher order functions
	- ex: array of struct
- higher order functions allows us to work directly with hierarchical data such as arrays and map type objects
- one common HOf is ***filter*** function
	- works similar to how filter works in python
	- filter an array using the given lambda function
```
SELECT order_id, multiple_copies
from (SELECT
order_id,
books,
filter(books, i -> i.quantity >= 2) as multiple_copies # produces empty arrays if no match
from orders )
WHERE size(multiple_copies) > 0; # length equivalent
```
- how does the above query work?
	- when the subquery has not been given any name?
	- the above query worked without any issues. but in which cases un named subqueries are allowed

```
SELECT order_id, books, TRANSform(books, b -> CAST(b.subtotal * 0.8 AS INT)) as subtotal_after_discount from orders;
```

- table created in one notebook, is accessible from another notebook given that both of these notebooks are attached to the same cluster
- UDfs: allows us to register a custom combination of SQL logic as function in a database
	- making these methods reusable in any SQL query
	- these leverage spark sql directly, ***maintaining all the optimization of Spark*** when applying our custom logic to large datasets
	- are these different from udfs defined programmatically using Java or python?
```
CREATE OR REPLACE function get_url(email STRING)
RETURNS STRING
RETURN concat("https://www.",split(email,"@")[1]);
SELECT email, get_url(email) from customers;
```
- udfs are permanent objects just like tables, we can use them bw different spark sessions and notebooks
	- use `DESCRIBE FUNCTION function_name;` command