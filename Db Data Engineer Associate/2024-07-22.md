![[Pasted image 20240722091432.png]]
- we can see where the function is stored
	- default in the above output signifies database under which the function is registered
![[Pasted image 20240722091655.png]]

```
CREATE FUNTION site_type(email STRING)
RETURNS STRING
RETURN CASE WHEN email like '%.org' then 'Non-profit org'
ELSE "Unknown domain" end;
```

- all the above functions are natively executed in spark, hence optimized for parallel execution
```
DROP FUNCTION function_name;
```

## Structured Streaming
- process streaming data
	- two approaches:
		- reprocess the entire dataset each time
		- only process those new data added since last update
			- Structured Streaming
				- automatically detects new data and persists the results incrementally into a data sink
				- treats the infinite input data stream as a table
					- new data considered to be new rows that are appended
					- called unbounded table
				- an input data stream could be a directory of files, a messaging system like kafka, or a delta table
				- delta lake is well integrated with spark structured streaming
- DataStreamReader
	- spark.readStream() to query the delta table as a streaming source
		- this allows us to process all of the data present in the table as well as any new data that arrive later
		- in the video he says that we can perform any transformation as if the SDF was a static df
- DataStreamWriter
	- df.writeStream to write to a durable storage
	- checkpoint location to track the progress of the stream processing
	- trigger: when the system should process next set of data
		- set by the trigger interval
		- default: 1/2 second
		- we can specify a ***fixed interval*** and process data in micro batches
		- ***once*** trigger interval to process all the available data in a single batch and stop
			- set by trigger(once = True)
			- all available data processed in a single batch
		- trigger(availableNow=True), process all available data in multiple microbatches, then stop
		- output modes
			- append default
				- new appended rows are incrementally appended to the target table with each batch
			- complete
				- result table is recalculated each time a write is triggered
				- the target table is ***overwritten*** each time
		- checkpoint
			- current state of the streaming job is stored in the cloud storage
			- streaming engine uses the this checkpoint to track progress
			- a checkpoint cannot be shared between several streams
			- a separate checkpoint location is required for every streaming write to ***ensure processing guarantees***
- structured streaming provides two guarantees:\
	- resumability
		- in case of failure
		- thanks to both checkpointing and a mechanism called Write-ahead-logs
		- allow to record the offset range of data being processed during each triggerd interval
		- to track our stream progress
	- exactly once data processing 

```
# DataStreamReader
streamDF = spark.readStream.table("Input_Table")

# DataStreamWriter
streamDF.writeStream
.trigger(processingTime ="2 minutes")
.outputMode("append")
.option("checkpointLocation","/path")
.table("Output Table")
```
