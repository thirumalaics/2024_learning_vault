## Delta Live Tables
- what is DLT?
	- fw for building reliable and maintainable data processing pipelines
	- simplifies building large scale ETL pipelines while maintaining table dependencies and data quality
	- DLT pipelines are implemented using DB notebooks
- how to create delta live tables?
	- DLT will always be preceded by the LIVE keyword
	- ```CREATE OR REFRESH STREAMING LIVE TABLE orders_raw COMMENT "xyz" AS SELECT * FROM cloud_files("${dataset.path}/orders-raw", "parquet", map("schema", "col1 type1, ..."))```
	- this is specifically for incremental processing via Auto loader
		- `STREAMING` keyword is required for Autoloader
	- cloud_files method enables Auto loader to be used natively with SQL
		- this method takes 3 arguments: data file path, data file format, an array of reader options(above this includes schema)
	- running the above statement does not populate the data
		- executing the above query only validates the syntax
		- we must create a pipeline in order to define and populate our table
	- ```CREATE OR REFRESH LIVE TABLE customers_raw COMMENT "xyz" AS SELECT from json.`path`;```
		- this query does not use auto loader
- how to reference DLTs and autoloader DLTs ?
	- `LIVE.table_name` 
	- for autoloader DLTs `STREAM(LIVE.table_name)`
- is there a way to perform quality control on the data that is streaming in?
	- for quality control, we can use constraint keyword
	- constraint keyword enables DLT to collect metrics on constraint violations
	- constraint keyword comes with an optional ON VIOLATION clause specifying an action to take on records that violate constraints
	- ![[Pasted image 20240727094132.png]]
	- in Omitted option, violation will be reported in the metrics
	- ```CREATE OR REPLACE STREAMING LIVE TABLE orders_cleaned (CONSTRAINT valid_order_number EXPECT(order_id is NULL) ON VIOLATION DROP ROW) COMMENT 'xyz' AS SELECT xxx, ... FROM STREAM(LIVE.orders_raw) o LEFT JOIN LIVE.customers c on o.customer_id = c.customer_id```
	- why is there a STREAMING keyword above? so does this table use autoloader?
		- may be implicitly as the orders_cleaned sources from a table that is loaded by auto loader
- how to create DLT pipeline?
	- put all the definitions in the a notebook
		- definition of tables and how they are sourced from each other as SQLs like above
		- Workflow tab in the DB can be used to create pipelines for DLTs
			- create pipeline
- what are the details asked in create pipeline menu?
	- notebook libraries: fill in the notebook path with table definitions
	- we can also provide params key value which is used in the notebook: ex - datasets.bookstore
	- storage location is also prompted which is the path where data files and pipeline logs will be stored
	- target field will ask for target dataset name
	- pipeline mode option defines how the pipeline will be run
		- there are two modes: Triggered, continuous
			- Triggered: run once and then shut down until next manual or scheduled updates
			- Continuous pipeline will continuously ingest new data as it arrives
		- new cluster will be created for our DLT pipeline
		- there are different cluster modes:
			- Fixed size is one mode
				- we set the num of workers