- SQL in spark is an abstraction over Df
- SQL is supported in Spark in two main areas
	- programmatically in expressions
	- in the spark shell
- database, table and views are present
- allowing access to df as a table
- Dataframe v table
	- identical in terms of storage and partitioning
	- dataframes can be processed programmatically, tables in sql
- `docker-compose up --scale spark-worker=3`
	- one master and 3 child nodes will be instantiated as a result of this command
- `docker exec -it spark-cluster-spark-master-1 bash`
	- will create a bash shell in the spark-cluster-spark-master-1 container
- table created with one session of spark-sql exists for the consecutive session
```
DESCRIBE EXTENDED table_name; // displays a lot of md associated with the tbale
```

- table creating is as follows:
	- `CREATE TABLE flights(origin string, destination string) using csv options(header true, path "/path/to/external/location");`
	- the path we give must exist already

```C:\Users\malai\IdeaProjects\spark-essentials```


```
carsDf.createOrReplaceTempView("cars")  
  
spark.sql(  
  """  
    |SELECT name from cars;
        |""".stripMargin).show()
```
- the strip margin function strips the blanks/control characters followed by the |
- spark.sql method always returns a df even if we run a create statement
	- creating a database using the sql passed to this method creates a spark-warehouse folder in our working directory
	- the directory or location under which the spark-warehouse folder is created is configurable
		- especially while instantiating the sparkSession
		- by default it will be in the project dir
		- configured by `spark.sql.warehouse.dir`
	- surprisingly running a create database statement without a show/action, created the database directory in the given location
		- not just the directory was created but also the db was visible as output of show databases statement
	- so far all the databases that I created are session scoped even though the folders are created for them
		- db that I created during one run is not visible at the start of the next run
- reading spark tables:
	- spark.read.table(tableName)
- writing tables: 
	- emp.write.mode(SaveMode.Overwrite).saveAsTable("employees")
		- default file format: Parquet
