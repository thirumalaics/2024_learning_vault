- datasets are typed dfs
	- distributed collections of jvm object
- dfs are distributed collections of type row
- to manipulate any data, so far we have been resorting to use methods provided for the dataframe object
	- what if we want to use predicate defined using a scala function
- to create a ds of single column
```
import org.apache.spark.sql.{Dataset, Encoders, SparkSession}
implicit val intEncoder = Encoders.scalaInt // exp that will return an encoder of int
val numbersDS: Dataset[Int] = df.as[Int] // this did not work
```

^6a8683

- an encoder of int has the capability of turning a row in df to an int
	- int will be the type of jvm objects of numbers
- the above may work out for a single column df
- we define a case class to define columns and their types as a jvm object
```
case class Car(
	Name: String,
	Miles_per_Gallon: Double,
	Cylinders: Long,
	Displacement: Double, 
	Horsepower: Long,
	Weight_in_lbs: Long,
	Acceleration: Double,
	Year: Date,
	Origin: String
)
implicit val carEncoder = Encoders.product[Car]

```
- Encoders product method takes a type parameter
	- any type that extends the product type
	- all case classes extend the product type 
- writing the carEncoder is not mandatory if I do the following: ^eedb75
```
import spark.implicits._
val carsDS = carsDf.as[Car] // this did not work
// because my stupid ass did not read with inferschema
```
- encoder will be automatically fetched from spark implicits
- any transformation on the datasets will return a new dataset
- datasets have the show method to display the results
```
case class Stocks (  
                      symbol: String,date: Option[String],price: Option[Double]  
                    )  
val stockEncoder = Encoders.product[Stocks]  
val stockSchema = stockEncoder.schema  
val stocksDf = spark.read.format("csv").option("header","true").schema(stockSchema).load("path")  

val stocksDS = stocksDf.as[Stocks](stockEncoder)  
stocksDS.printSchema()
```

- performance is poor with datasets
	- row by row execution
	- all transformations and filters are evaluated at runtime after spark has the chance to plan for the operations in advance
- ds does not come with fold method
- the Option class is used above to deal with nulls, we are indicating to expect nulls
- to use Option typed columns in filters:
```
carsDS.filter(x => x.Horsepower.getOrElse(0L) > 140)
carsDS.map(_.Horsepower.getOrElse(0L)).reduce((_ + _)) / dsCount
```
- datasets still have access to all df api methods

https://stackoverflow.com/questions/45414718/spark-scala-cannot-up-cast-from-string-to-int-as-it-may-truncate