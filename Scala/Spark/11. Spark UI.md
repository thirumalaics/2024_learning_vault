- spark UI shows all the jobs that are complete and in progress
- UI also displays number of tasks, along with the number of successful tasks, failed tasks(if any) and tasks in progress (if any) as well
- there are event timelines at a job and stage level
- job level event timeline shows events related to the executors(added, removed) and the jobs

- stage view
	- shows additional info about ip data size for each stage, shuffle read and write data size, and number of tasks
	- skipped stages are also mentioned
	- even timeline shows time taken by each task to complete, or time spent so-far by ongoing tasks
		- we can see when a particular task started, completed and we can compare these metrics bw tasks
	- timeline of each task is broken down by computation time and time taken by administrative tasks such as serialization/de-serialization, shuffle read/write etc.
	- per stage we can also see the summary metrics of tasks for that stage
		- ex: duration of tasks(min, max, median etc..), GC Time (min, max etc..)
		- input size/Records -> Min, max, median etc..
		- shuffle write size/records -> min max, median etc...
		- ideally all durations and data sizes should be close to median as much as possible
	- at the bottom of this page every task's metrics are displayed as part of the table
		- time take for every task to complete, which node it ran on, GC Time, input size, shuffle size is available
		- also links to console outputs for each task, both stderr and stdout are also present and can be used to assess the outputs
- there is also a SQL/Df view
	- can be accessed by selecting the SQL/Df button on the top bar
		- for each query a dag visualization is available
	- queries are displayed with the job ids associated with them
		- query ID is different from job id
- storage view
	- keeps all the data that is persisted
		- ex: cached df
	- size of the persisted data in memory and on disk is also displayed
		- size detail is displayed for all the blocks of the cached dataframe
- env view
	- shows the configured env
- executors view
	- executors view displays all executors, as well as summary breakdown
		- along with driver related metrics
	- shows the number of active, completed and failed tasks for each executor instance, as well as GC time, input and shuffle data sizes and if available, logs of each executors
- spark master UI
	- available for clusters
	- displays all applications that are running or completed in the cluster
	- link for an application level spark ui is present for each application
https://medium.com/@suffyan.asad1/beginners-guide-to-spark-ui-how-to-monitor-and-analyze-spark-jobs-b2ada58a85f7