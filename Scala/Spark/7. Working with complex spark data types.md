- I think the spark date parsing fails when the num of chars we have given in our format string is greater than the num of characters in a column value
	- movies json in the spark-essentials project proves this
- spark, when not able to parse the date or time as per the given format returns nulls
	- in all cases except the first mentioned above
- there is a struct function that accepts column objects as arguments and returns a struct
	- few ways to retrieve a field from a struct:
		- `col("Struct_Col").getField("Struct_Field").as("My_Struct_Column")`
		- `df3.select("Struct_Col.Struct_Field").show()`
- how to make a struct using expression string:
	- `(col1, col2) as struct_col`
- creating arrays from existing string column:
	- `df.withColumn("Name_Constituent",split(col("Name"), " |-")).select("Name_Constituent").show()`
- to access element of an array type column
	- `selectExpr("Array_Column[0]")`
	- `df.select(col("Name_Constituent").getItem(0).alias("Hey"))`
- col("Name_Constituent").contains
	- only useful for string columns not for array cols
	- for array cols: array_contains