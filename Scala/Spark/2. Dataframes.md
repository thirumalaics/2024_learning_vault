- spark session
```
import org.apache.spark.sql.SparkSession

object myOb extends App{
	val spark = SparkSession.builder().appName("myApp").config("spark.master", "local").getOrCreate() 
}
```

- we have to provide the spark.master for the scala code to work, without it I was getting weird error
- [spark.master](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls) determines the cluster manager to connect to

```
spark.read.format("json").option("inferSchema", "true").load(path)
```
- foreach methods can be called on collections
	- `firstDF.take(5).foreach(println)`
- spark types such as LongType are singleton objects that spark uses internally to describe the data
- to describe a dataframe of multiple columns, we have the type StrutType
	- contains an Array of StructFields, which themselves represent columns
```
import org.apache.spark.sql.types.{StringType, StructField, StructType, DoubleType}
val stocksSchema = StructType(Array(  
  StructField("symbol", StringType),  
  StructField("date", StringType),  
  StructField("price", DoubleType)  
))
// to obtain schema from a df:
val carsDFSchema = firstDF.schema
```
- creating rows manually
	- `val mySampleRow = Row("Thirumalai S", 25)`
- we can create a dataframe from a sequence of rows/tuples
	- createDataFrame method is heavily overloaded
	- when providing a sequence of tuple as the data, we are not allowed to provide a schema object
		- because the types are known at compile time with scala
		- this comes at a disadvantage, we are not able to provide column name
```
val data = Seq(  
  (1, "Nayanu", 26),  
  (2, "Niyu", 15),  
  (3, "Thirumalai S", 25)  
)  
val dataframe = spark.createDataFrame(data)
// df created with column names _1, _2, _3....
```
- schemas are not required when creating a row object
- there is another way to create dataframes manually
```
import spark.implicits._

val manualDFWithImplicits = data.toDF("id", "Name", "age") // returns a df
```

- the schema that we provide known at execution time and not at compile time, that is where datasets differ
- to import everything inside a package or module:
	- `import org.apache.spark.sql.types._`
- when the failFast mode is used for read and there are malformed records, if I use count as an action to see if the read happens, the action is successful even though there are malformed records
	- the malformed records are not ignored during count
- but the same behavior is not seen when I do show, an error is throw because of the malformed records
- reading with options:
```
val dfWithOptions = spark.read.format("json")  
  .options(Map(  
    "path" -> "C:\\spark-essentials\\src\\main\\resources\\data\\cars.json", 
    "mode" -> "failFast",  
    "inferSchema" -> "true"  
  ))  
  .load()
```
- use SaveMode.Overwrite or other respective mode specifier while writing
- crc files after write can be used to validate the integrity of the other files