- spark session
```
import org.apache.spark.sql.SparkSession

object myOb extends App{
	val spark = SparkSession.builder().appName("myApp").config("spark.master", "local").getOrCreate() 
}
```

- we have to provide the spark.master for the scala code to work, without it I was getting weird error
- [spark.master](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls) determines the cluster manager to connect to

```
spark.read.format("json").option("inferSchema", "true").load(path)
```
- foreach methods can be called on collections
	- `firstDF.take(5).foreach(println)`
- spark types such as LongType are singleton objects that spark uses internally to describe the data
- to describe a dataframe of multiple columns, we have the type StrutType
	- contains an Array of StructFields, which themselves represent columns
```
import org.apache.spark.sql.types.{StringType, StructField, StructType, DoubleType}
val stocksSchema = StructType(Array(  
  StructField("symbol", StringType),  
  StructField("date", StringType),  
  StructField("price", DoubleType)  
))
// to obtain schema from a df:
val carsDFSchema = firstDF.schema
```
- creating rows manually
	- `val mySampleRow = Row("Thirumalai S", 25)`
- we can create a dataframe from a sequence of rows/tuples
	- createDataFrame method is heavily overloaded
	- when providing a sequence of tuple as the data, we are not allowed to provide a schema object
		- because the types are known at compile time with scala
		- this comes at a disadvantage, we are not able to provide column name
```
val data = Seq(  
  (1, "Nayanu", 26),  
  (2, "Niyu", 15),  
  (3, "Thirumalai S", 25)  
)  
val dataframe = spark.createDataFrame(data)
// df created with column names _1, _2, _3....
```
- schemas are not required when creating a row object
- there is another way to create dataframes manually
```
import spark.implicits._

val manualDFWithImplicits = data.toDF("id", "Name", "age") // returns a df
```