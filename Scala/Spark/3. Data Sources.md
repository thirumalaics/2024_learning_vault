- when the file has a date column encoded as a string, spark will try to parse the string in a format which can be configured
```
val df = spark.read.format("json")
.schema(schema)
.options(Map(
"dateFormat" -> "YYYY-MM-dd", // the upper case Y works only in spark 2
"allowSingleQuotes" -> "true",
"timestampFormat" -> "yyyy-MM-dd HH:mm:ss"
).load(path)
```
- the date format specifier only works with enforced schema
	- otherwise spark would not know which all columns need to be checked for the date
- if dateformat is not specified std ISO dateformat is specified
	- this dateformat option is not available for data sources such as parquet
	- if the parsing is unsuccessful, the null value is used
	- dateformat option only applicable for the date type
- timestamp formats can be specified by timestampFormat
- if we have a json formatted with single quotes, we can use "allowSingleQuotes", "true"
```
val df = spark.read
.schema(schema)
.options(Map(
"dateFormat" -> "YYYY-MM-dd", // the upper case Y works only in spark 2
"allowSingleQuotes" -> "true",
"timestampFormat" -> "yyyy-MM-dd HH:mm:ss"
).json(path)
```

- when dealing with csv files atleast, the headers do not have to match what we columns names provided in the schema
- nullValue option to specify what value in the csv file must be parsed as null
	- csv as a file format does not have the concept of nulls
- if we want to write files in parquet format, we do not have to provide format
	- `df.write.mode(SaveMode.Overwrite).save("path")`
- surprisingly text file read is possible using spark
	- `spark.read.text()`
	- each line is treated as a row