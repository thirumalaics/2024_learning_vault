![[Pasted image 20240911085843.png]]
- all commands and queries go to the driver which compiles the input, optimizes the computation required, and executes the required steps usually with mapreduce jobs
- when mr jobs are required, hive does not generate Java MR programs
	- it uses built-in generic Mapper and Reducer modules that are driven by an XML file representing job plan
- hive communicates with the jobtracker to initiate the MR Job
- hive does not have to be running on the same master node with the jobTracker
- in large clusters, it is common to have edge nodes where tools like Hive run
- hive communicates remotely with the JobTracker on the master node to execute jobs
- usually the data files to be processed are in HDFS, which is managed by NameNode
	- Name Node: keeps the directory tree of all files in the file system
		- tracks where across the cluster the file data is kept
		- it does not store the data itself
- metastore is a separate rdb(usually a mySQL instance) where hive persists table schemas and other system md
- hive is best suited for dwh applications where we do not need: 
	- real-time responsiveness to queries
	- record-level inserts, updates and deletes are not required
- pig is an alternative to Hive
	- used more in cases where external data is pulled and transformed in a cluster
	- uses a custom language not based on SQL
- for row-level updates, rapid query response times and txns: HBase
	- supports row-level txns(but not multirow txns)
	- kv store where each key is used for each row to provide very fast reads and writes of the row's columns or column families

## Hadoop Distribution modes
- default = local
	- filesystem references use the local file system
	- map and reduce tasks are run as part of the same process
		- including most hive queries
	- all jobs run in a single jvm instance
- distributed
	- actual clusters configured in this way
	- all filesystem references that are not full URI default to the distributed filesystem
	- jobs are managed by JobTracker service, with individual tasks executed in separate processes
		- jobtracker is a hadoop servive that farms out MapReduce tasks to specific nodes in the cluster
- pseudodistributed mode
	- behavior is identical to distributed mode
	- filesystem references default to distributed filesystem
	- jobs are managed by JobTracker Service
	- HDFS file block replication is limited to one copy
	- behavior is like a single-node cluster
- even when running in distributed mode, hive can decide on a per-query bases whether or not it can perform the query using the local mode
	- where hive manages the MR jobs

## What is inside Hive?
- multiple JAR files
- each jar implements a particular subset of hive's functionality
- $HIVE_HOME/bin directory contains executable scripts that launch various hive services
	- including the hive CLI
	- CLI can be used to run individual lines of commands and also scripts of hive statements
	- other components include:
		- thrift service: remote access from other processes
		- access using JDBC and ODBC are provided too
			- implemented on top of the thrift service
- all hive installations require a ms service
	- used to store table schemas and other md
	- typically implemented by a RDB
		- by default built-in derby SQL server
		- which provides limited single-process storage
			- when using derby, we cannot run two simultaneous instances of CLI
			- a folder metastore_db created when running with derby in the current working dir
			- derby forgets old metastore_db folders that it created, so if we change the cwd,  a new metastore_db is created
			- this is the folder that contains md about our tbl
		- MySQL or similar dbs are reccoed for real use cases
- a hive web interface
	- provides remote access to hive
- conf dir contains files that configure hive
- case is ignored by hive
![[Pasted image 20240911093557.png]]


https://inthiraj1994.medium.com/review-on-existing-distributed-file-system-1d78ad0a9a5d