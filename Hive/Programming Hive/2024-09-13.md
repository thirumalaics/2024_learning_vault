- if we plan to use the local mode regularly, it is worth configuring a std location for the derby ms db
	- remember this is different from the dir used to store table data(default user/hive/wareshouse)
	- config can be changed in hive-site.xml

- hadoop 1.x has different daemons running
	- NameNode
	- Secondary NameNode
	- DataNode
	- JobTracker
	- TaskTracker
- each daemon conducts its operations autonomously within its JVM
	- each of the above are java processes
- nameNode: master daemon and is responsible for storing all location information of the files present in HDFS
	- actual data not stored in nameNode
	- all md help in RAM, hence quick response to read requests
	- important to run this daemon in a node with lots of RAM
	- higher the number of files in HDFS, higher the consumption of RAM
	- namenode maintains a persistent checkpoint of md in a file stored on the disk called fsimage file
	- whenever a file is placed, deleted or updated in the cluster an entry of this action is updated to a file called edits log file
	- then the md in mem is updated
	- important to note that fsimage is not updated for every action that takes place
	- in case the nameNode is restarted, the following sequence of events occur at namenode boot up:
		- read the fsimage file from disk and load it into mem
		- read actions in the edits log and apply them to in-mem repr of the fsimage file
		- write the modified in-mem repr to the fsimage file on disk
	- the preceding steps make sure that the in-mem repr is up to date
	- the namenode daemon is a single point of failure
		- if the node hosting this daemon fails, filesys becomes unusable
		- to handle this, the admin must configure the namenode to write the fsimage file to the local disk as well as a remote disk on the nw
		- this backup on the remote disk can be used to restore the nameNode on a fresh server
	- Hadoop 2.X now support High Availability, which deploys two namenodes in an active/passive configuration
		- if the active nameNode fails, the control falls onto the passive namenode
	- it is possible that the edits may grow large and fsimage might fall way behind
		- in this case, the namenode boot up becomes slow
			- as nameNode has to apply new actions in edits file to the in-mem repr
		- the slow bootup time can be avoided using the secondary nameNode daemon
	- responsible to maintain a list of files and their corresponding locations on the cluster
	- whenever a client wants to access a file, namenode daemon provides the location of the file to client
		- client access from the datanode daemon
- secondary nameNode
	- responsible for periodic housekeeping fns for the namenode
	- not a failover for the namenode daemon
	- recommended to be hosted on a separate machine for large clusters
	- creates a checkpoint of the filesystem md present in namenode by merging the edits logfile and fsimage
		- the checkpoint created by this snn could be used upon bootup
	- since checkpoints are done in intervals, it is possible that the checkpoint data could be slightly outdated
		- may cause data loss
	- following are the steps carried out by SNN
		- get the edits an fsimage files from PNN
		- apply all actions present in edits to the fsimage
		- push the fsimgae back to PNN
	- PNN would have relatively updated version of fsimage
	- i think it is still preferable to have PNN do the update after bootup
		- this time the num of updates should be less
![[Pasted image 20240913090209.png]]

- data node
	- daemon acts as a slave node and is responsible for storing the actual files in HDFS
	- files are split as data blocks across clusters
		- size: 64MB to 128MB
	- block size is a configurable parameter
	- file blocks are replicated to other datanodes for redundance so that no data is lost in case a datanode daemon fails
	- data
	- remember that datanode is a process, they do not hold data imo
		- they are just responsible for storing and managing the data stored in a node
	- if a datanode is lost or hdfs is shut down, data can be found in the [configured directory in local filesystem](https://stackoverflow.com/questions/28379048/data-lost-after-shutting-down-hadoop-hdfs)
	- datanode daemon sends information to the namenode daemon about the files and blocks stored in that node and responds to the namenode daemon for all filesys operations
![[Pasted image 20240913091816.png]]

- jobtracker
	- accepts job requests from a client
	- schedules/assigns tasktrackers with tasks to be performed
	- job tracker daemon tries to assign tasks to tasktracker daemon on the data node daemon where the data to be processed is stored
		- this is called data locality
		- if this assignment is not possible, tries to assign tasks to taskstrackers within the same physical server rack
		- if the node hosting the datanode and tasktracker daemon fails, the jobtracker daemon assigns the task to another tasktracker daemon where replica exists
		- this ensures job does not fail even if a node fails
https://subscription.packtpub.com/book/cloud-and-networking/9781783558964/1/ch01lvl1sec10/understanding-the-apache-hadoop-daemons
