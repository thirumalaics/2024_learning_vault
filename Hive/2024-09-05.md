- distribute by
	- hive uses distribute by to distribute kvp amongst the reducers
	- decides which keys will go to which reducer
	- all rows with same key column values will go to the same reducer
	- each reducer gets non-overlapping range of values
	- not sure how many reducers will be there in total
	- distribute by does not do any sorting, it just distributes the rows based on the key column values
	- `SELECT col2 FROM table5 DISTRIBUTE BY col2;`
	- to do sorting, we need to use sort by along with distribute by
	- sort by will do sorting within the reducer to get the output
	- `SELECT col2 FROM table5 DISTRIBUTE BY col2 SORT BY col2`
	- I believe the columns given to both of these clauses must be same #doubt 
	- is there any limit on the number columns to be given to both of these clauses
- cluster by
	- kind of an alias of DISTRIBUTE BY SORT BY
	- logic wise no diff
	- `SELECT Col2 from table5 cluster by col2;`
## Date and Math functions
- unix_timestamp('2024-09-05 00:00:00')
	- seconds elapsed since 1970-01-01 00:00:00
- from_unixtime(12345678)
	- returns a string representing given time elapsed since the unix epoch
- to_date, Month, Year, day,dayofmonth, hour, minute, second, weekofyear, datediff,date_add, date_sub
- ceil(9.5), floor(9.5), round(123.456,2), rand()

## String functions
- concat(col1, ',',col3), length(col3), lower, upper,lpad(col3,total_length_of_chars, 'v'), rpad, trim, ltrim,rtrim,repeat(col, num of times to be repeated),reverse(col3)
- split('hive:hadoop',':') = \['hive','hadoop'\]
	- splits a string based on the given delimiter string
	- returns an array of strings just like in spark
- substr
	- 1 based indexes
	- substr('hive is querying tool',4)
	- outputs: e is querying tool
	- substr('hive is querying tool',4,1)
	- outputs: e
- instr
	- instr('hive is querying tool', 'e')
	- returns the index of the search string e, in they above it outputs 4
	- 1 based index
## Conditional statements
- if
	- if(col3='England', col1(if true), col4(if false))
- case when
	- just like in other sql dialects there are two ways of defining case when statements
	- `CASE col1 WHEN 'Apple' THEN 'The owner is APPLE' WHEN 'ORANGE' THEN 'The owner is Orange' ELSE 'It is another fruit' END`
	- `CASE WHEN col1 = 'APPLE' THEN 'APPLY' WHEN col1= 'Orange' THEN 'The owner is orange' ELSE 'It is another Fruit' END`
- isnull
	- returns a boolean based on the value of the given column
	- isnull(col2)
- isnotnull(col1)
- coalesce
	- returns the first non-null value in the given expressions
	- coalesce(datefield1, datefield2, datefield3)
- nvl
	- only two args
	- returns the first non-null value
	- NVL(col1, col2)

## Explode and lateral view
- explode same as in spark, takes in an array type column, returns a row for each value in the array
	- but limitation of the explode function is that in the select statement only the column to be exploded can be selected
	- no other column can be selected
- lateral view
	- lateral view will create a virtual table for the exploded column
	- the virtual table is the joined with the base table to get the desired output
	- `select author_name, dummy_booksname from table2 lateral view explode(books_name) dummy as dummy_booksname;`
	- the virtual table's name is dummy
	- dumm_booksname is column name
	- one more use of lateral view is to separate keys and values to separate columns from a ip map data type
	- `select key, value from table3 lateral view explode(col) dummy as key,value;`
![[Pasted image 20240905091441.png]]

## Rlike
- any substring of a give column matches with the given search string, the value returned is true
- `SELECT 'hadoop' rlike 'ha';`
	- returns true
- the search string accepts any string that follows java regex stds
	- `SELECT 'hadoop' rlike 'ha*';`
- rlike command does not need any trim function before hand
- if the given string or search string is null, output is null

- window functions
	- row_number, rank and dense_rank
	- if no partition by given, only one reducer is used