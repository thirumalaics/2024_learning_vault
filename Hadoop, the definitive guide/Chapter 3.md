- distributed file system: filesystems that manage the storage across nw of machines are called distributed filesystems
- distributed file systems handle complexities that arise when we store data across machines
	- these complexities are mainly related to the nw
	- one complexity is to make the fs tolerate node failure without suffering data loss
- Hadoop comes with a Dfs called HDfS
- hadoop also has a general purpose filesystem abstraction -> more on this later
- HDfS is designed for storing very large files with streaming data access patterns, running on clusters of commodity hw
	- dissecting the above:
		- streaming data access
			- HDfs built around the idea that most efficient data processing pattern is write once read many times
			- analyses involve the entire dataset, typically, hence the focus here is to optimize throughput of reading the entire data rather than latency in reading the first record
		- commodity hardware
			- does not require expensive, highly reliable hardware
			- commodity hw: commonly available hw that can be obtained from multiple vendors
			- chance of failure across the cluster is very high
			- HDfs is designed to carry on working without noticeable interruption to the user
- applications for which HDfS is not a good choice:
	- low-latency data access
		- HDfS optimized for delivering a high throughput of data, and this may be at the expense of latency
		- why can we not serve data with high throughput and low latency?
			- it is challenging to achieve this because of trade-offs in system design, architecture, physical limitations
			- HDfS replicates data, serving data from multiple replicas adds coordination overhead and delays
			- HDfS has large block size, which is useful to minimize metadata and maximize throughput for sequential reads/writes but even to read small piece of data entire block need to be transferred
				- bad for random reads
			- high throughput relies on streaming large chunks of data over the nw bw distributed nodes
				- nw comms introduces delays especially when a request involves retrieving data from multiple nodes
			- designing a system that delivers both high throughput and low latency require:
				- increased hw costs
					- SSDs, more mem, low latency nws
				- complex architecture
					- sophisticated indexing and caching mech
	- lots of small files
		- namenode holds md in mem
		- more files, then more md, then more load on namenode
		- each file, dir and block takes about 150 bytes in mem
	- multiple writers, arbitrary file modifications
		- files in HDfs may be written to by a single wrtier
		- writes are always made at the end of the file, in append-only fashion
		- no support for multiple writers or for modifications at arbitrary offsets in the file

## HDfS concepts
- a ***disk*** has a block size, which is the minimum amount of data that can read or written
	- in the order of bytes
- file system builds on the disk block size, by having it's own block size: file system block size
	- this block size is a integral multiple of disk block size
	- in the order of kbs
- for fs users, this is not very useful, but there are tools such as df and fsck which work on the filesystem block level
	- these are used to perform file system maintenance
- HDfS has it's concept of block
	- 128 MB by default
	- files in HDfS are broken into block-sized chunks, which are stored as independent units
	- a file in HDfS that is smaller than the block size does not occupy full block's work of underlying storage
		- the opposite is true for a file system for a single disk, files with size less than the file system block size occupy the full space
		- the unused space cannot be used for any other storage
- Why use filesystem blocks?
	- simplify how the data is stored and managed
	- blocks are used to allocate space for files and directories
		- fs has to manage fs blocks instead of each and every byte
		- fs md keeps track of blocks rather than individual bytes
	- minimizes the number of seek operations or access operations(SSD) needed to read or write a file
		- rather than seeking each byte of a file
	- blocks allow a file system to support large files without requiring continuous storage on the disk
	- blocks allow fs to maintain a bitmap or a list of free blocks to track unused space
	- if part of a file becomes corrupted, only the affected blocks are lost, not the entire file. makes recovery easier
- why is the block size in HDfS larger than normal file system?
	- for big data, especially when we have data split in to large files, if the block size were to be in KBs:
		- per file there will have to be many seek operations
		- time spent seeking is time not spent transferring(by making use of the disk transfer rate)
		- by making the block size large, we avoid spending more time in seek and spend more time transferring the data
- benefits of block abstraction for a dfs:
	- a file can be larger than any single disk in the nw
		- a file can be split and stored across the nw
	- making the unit of abstraction a block rather than a file simplifies the storage subsystem
		- does not have to deal variable sizes
		- minimizes fragmentation(I believe 128Mb block size saves from external fragmentation):
			- external fragmentation: when a piece of data is not stored in a contiguous space in disk
				- blocks are scattered across disk
				- inefficient as the file system has to piece together data which is an overhead
			- internal fragmentation:
				- piece of data does not fully utilize the block size provided
	- fits well with replication for providing fault tolerance and availability
		- blocks that make up the file are replicated by replication factor across physically separate machines
		- if a block becomes unavailable, a copy can be read from another location
		- a block that is unavailable due to corruption or machine failure can be replicated from its alternate locations to bring back the replication factor to normal
		- `hdfs fsck / -files -blocks`
			- lists the blocks that make up each file in the filesystem
- md of a file is stored stored with each an every block
	- md is centralized or in other words managed by a diff entity called name node server
		- data distributed, md centralized
		- separating md helps in modular approach
			- where as in normal file system it is integrated as the data is not huge and designed for a single machine
	- even in normal file system md is stored separately using ds like inodes
		- md tightly integrated in to file system itself unlike hadoop
		- md stored on same disk but separately
		- a file system driver is responsible for managing both data and metadata

## Namenodes and Datanodes
- HDfS cluster has two types of nodes operating in a master-worker pattern
- name node - master, data nodes - workers
- the name node manages the filesystem namespace
	- maintains the filesystem tree and md for all the files and directories
		- this information stored persistently on the disk in the form of 2 files: the namespace image and the edit log
	- name node also knows the location in data nodes of all blocks that constitute to all files in HDfS
		- not stored persistently
		- this information is reconstructed from datanodes every time the system starts
- a client is created whenever we want to interact with the HDfS
	- in most cases it is immediately destroyed after use, especially when we are issuing commands using CLI
		- we can also explicitly create and destroy, in which case we have the control of its lifecycle
	- this client is instantiated on the machine where the code runs
		- a dev machine if we are trying to access a remote HDfS cluster
		- a node in the hadoop cluster, ex: edge node
		- usually the client talks to the namenode first as it needs metadata figured out
		- the client presents a filesystem interface similar to a POSIX(Portable System Interface), so the user code does not need to know about the namenode and datanode
- What is POSIX? 
	- set of standardized operating system interfaces that determine how sw interacts with an operating system's core services
		- ex: file operations, process management, inter process communication
	- these standards ensure compatibility and portability of applications across OSs
- data nodes are the workhorses of the filesystem
	- they store and retrieve blocks when they are asked to(by clients or namenodes)
	- they report back to name node periodically with lists of blocks that they are storing
- without the namenode, the filesystem cannot be used
	- because only name node knows what  blocks to piece together across data nodes that constitute a file
- what is a namespace image?
	- it is a file that contains persistent checkpoint of the entire HDfS namespace
	- stores the state of a HDfS at a specific point in time
	- stored in the namenode usually, but in high-availability settings, it is stored in another machine(secondary or backup namenode)
- what is edit log?
	- records incremental changes made to the namespace since the last checkpoint
- the namespace image and edit log are periodically merged
- to ensure resilience of name nodes, hadoop provides two mech
	- back up the files that make up the persistent state of the filesystem metadata
		- hadoop can be configured to write its persistent state to multiple filesystems
			- synchronous and atomic
			- usual config is to write to local disk and a NfS mount
	- it is also possible to run a secondary name node, which despite the name, does not act as a name node
		- main role is to periodically merge the namespace image and edit log to prevent the edit log from becoming too large
		- runs on a separate machine as it requires plenty of CPU and as much memory as the namenode to perform the merge
## Block Caching
- normally a data node reads blocks from disk
	- but for frequently accessed files ***the blocks*** may be explicitly cached in the data node's off-heap block cache mem
	- by default, a block is cached in only one data node's memory, configurable on a per-file basis
	- users or applications instruct the name node on which files to cache(and for how long) by adding a cache directive to a cache pool
		- cache pools are an administrative grouping for managing cache permissions and resource usage

## HDfS federation
- for large clusters with many files, scaling might become difficult with a single namenode
- HDfS federation was introduced in Hadoop 2.x
	- allows a cluster to scale by adding namenodes
	- each of which manages a portion of the filesystem namespace
		- ex: one namenode manages files rooted under /user
- under federation, each nodenode manages
	- a namespace volume, which is made up of the metadata for the namespace
		- independent of each other, name nodes dont comms with each other
		- failure of one does not affect others
	- block pool containing all the blocks for the files in the namespace
		- block pool storage not partitioned
		- so datanodes register with each namenode in the clusterr and store blocks from multiple block pools
	- 