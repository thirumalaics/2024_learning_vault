- distributed file system: filesystems that manage the storage across nw of machines are called distributed filesystems
- distributed file systems handle complexities that arise when we store data across machines
	- these complexities are mainly related to the nw
	- one complexity is to make the fs tolerate node failure without suffering data loss
- Hadoop comes with a Dfs called HDfS
- hadoop also has a general purpose filesystem abstraction -> more on this later
- HDfS is designed for storing very large files with streaming data access patterns, running on clusters of commodity hw
	- dissecting the above:
		- streaming data access
			- HDfs built around the idea that most efficient data processing pattern is write once read many times
			- analyses involve the entire dataset, typically, hence the focus here is to optimize throughput of reading the entire data rather than latency in reading the first record
		- commodity hardware
			- does not require expensive, highly reliable hardware
			- commodity hw: commonly available hw that can be obtained from multiple vendors
			- chance of failure across the cluster is very high
			- HDfs is designed to carry on working without noticeable interruption to the user
- applications for which HDfS is not a good choice:
	- low-latency data access
		- HDfS optimized for delivering a high throughput of data, and this may be at the expense of latency
		- why can we not serve data with high throughput and low latency?
			- it is challenging to achieve this because of trade-offs in system design, architecture, physical limitations
			- HDfS replicates data, serving data from multiple replicas adds coordination overhead and delays
			- HDfS has large block size, which is useful to minimize metadata and maximize throughput for sequential reads/writes but even to read small piece of data entire block need to be transferred
				- bad for random reads
			- high throughput relies on streaming large chunks of data over the nw bw distributed nodes
				- nw comms introduces delays especially when a request involves retrieving data from multiple nodes
			- designing a system that delivers both high throughput and low latency require:
				- increased hw costs
					- SSDs, more mem, low latency nws
				- complex architecture
					- sophisticated indexing and caching mech
	- lots of small files
		- namenode holds md in mem
		- more files, then more md, then more load on namenode
		- each file, dir and block takes about 150 bytes in mem
	- multiple writers, arbitrary file modifications
		- files in HDfs may be written to by a single wrtier
		- writes are always made at the end of the file, in append-only fashion
		- no support for multiple writers or for modifications at arbitrary offsets in the file

## HDfS concepts
- a ***disk*** has a block size, which is the minimum amount of data that can read or written
	- in the order of bytes
- file system builds on the disk block size, by having it's own block size: file system block size
	- this block size is a integral multiple of disk block size
	- in the order of kbs
- for fs users, this is not very useful, but there are tools such as df and fsck which work on the filesystem block level
	- these are used to perform file system maintenance
- HDfS has it's concept of block
	- 128 MB by default
	- files in HDfS are broken into block-sized chunks, which are stored as independent units
	- a file in HDfS that is smaller than the block size does not occupy full block's work of underlying storage
		- the opposite is true for a file system for a single disk, files with size less than the file system block size occupy the full space
		- the unused space cannot be used for any other storage
- Why use filesystem blocks?
	- simplify how the data is stored and managed
	- blocks are used to allocate space for files and directories
		- fs has to manage fs blocks instead of each and every byte
		- fs md keeps track of blocks rather than individual bytes
	- minimizes the number of seek operations or access operations(SSD) needed to read or write a file
		- rather than seeking each byte of a file
	- blocks allow a file system to support large files without requiring continuous storage on the disk
	- blocks allow fs to maintain a bitmap or a list of free blocks to track unused space
	- if part of a file becomes corrupted, only the affected blocks are lost, not the entire file. makes recovery easier
- why is the block size in HDfS larger than normal file system?
	- for big data, especially when we have data split in to large files, if the block size were to be in KBs:
		- per file there will have to be many seek operations
		- time spent seeking is time not spent transferring(by making use of the disk transfer rate)
		- by making the block size large, we avoid spending more time in seek and spend more time transferring the data
- benefits of block abstraction for a dfs:
	- a file can be larger than any single disk in the nw
		- a file can be split and stored across the nw
	- making the unit of abstraction a block rather than a file simplifies the storage subsystem
		- does not have to deal variable sizes
		- minimizes fragmentation(I believe 128Mb block size saves from external fragmentation):
			- external fragmentation: when a piece of data is not stored in a contiguous space in disk
				- blocks are scattered across disk
				- inefficient as the file system has to piece together data which is an overhead
			- internal fragmentation:
				- piece of data does not fully utilize the block size provided
	- fits well with replication for providing fault tolerance and availability
		- blocks that make up the file are replicated by replication factor across physically separate machines
		- if a block becomes unavailable, a copy can be read from another location
		- a block that is unavailable due to corruption or machine failure can be replicated from its alternate locations to bring back the replication factor to normal
		- `hdfs fsck / -files -blocks`
			- lists the blocks that make up each file in the filesystem
- md of a file is stored stored with each an every block
	- md is centralized or in other words managed by a diff entity called name node server
		- data distributed, md centralized
		- separating md helps in modular approach
			- where as in normal file system it is integrated as the data is not huge and designed for a single machine
	- even in normal file system md is stored separately using ds like inodes
		- md tightly integrated in to file system itself unlike hadoop
		- md stored on same disk but separately
		- a file system driver is responsible for managing both data and metadata

## Namenodes and Datanodes
- HDfS cluster has two types of nodes operating in a master-worker pattern
- name node - master, data nodes - workers
- the name node manages the filesystem namespace
	- maintains the filesystem tree and md for all the files and directories
		- this information stored persistently on the disk in the form of 2 files: the namespace image and the edit log
	- name node also knows the location in data nodes of all blocks that constitute to all files in HDfS
		- not stored persistently
		- this information is reconstructed from datanodes every time the system starts
- a client is created whenever we want to interact with the HDfS
	- in most cases it is immediately destroyed after use, especially when we are issuing commands using CLI
		- we can also explicitly create and destroy, in which case we have the control of its lifecycle
	- this client is instantiated on the machine where the code runs
		- a dev machine if we are trying to access a remote HDfS cluster
		- a node in the hadoop cluster, ex: edge node
		- usually the client talks to the namenode first as it needs metadata figured out
		- the client presents a filesystem interface similar to a POSIX(Portable System Interface), so the user code does not need to know about the namenode and datanode
- What is POSIX? 
	- set of standardized operating system interfaces that determine how sw interacts with an operating system's core services
		- ex: file operations, process management, inter process communication
	- these standards ensure compatibility and portability of applications across OSs
- data nodes are the workhorses of the filesystem
	- they store and retrieve blocks when they are asked to(by clients or namenodes)
	- they report back to name node periodically with lists of blocks that they are storing
- without the namenode, the filesystem cannot be used
	- because only name node knows what  blocks to piece together across data nodes that constitute a file
- what is a namespace image?
	- it is a file that contains persistent checkpoint of the entire HDfS namespace
	- stores the state of a HDfS at a specific point in time
	- stored in the namenode usually, but in high-availability settings, it is stored in another machine(secondary or backup namenode)
- what is edit log?
	- records incremental changes made to the namespace since the last checkpoint
- the namespace image and edit log are periodically merged
- to ensure resilience of name nodes, hadoop provides two mech
	- back up the files that make up the persistent state of the filesystem metadata
		- hadoop can be configured to write its persistent state to multiple filesystems
			- synchronous and atomic
			- usual config is to write to local disk and a NfS mount
	- it is also possible to run a secondary name node, which despite the name, does not act as a name node
		- main role is to periodically merge the namespace image and edit log to prevent the edit log from becoming too large
		- runs on a separate machine as it requires plenty of CPU and as much memory as the namenode to perform the merge
## Block Caching
- normally a data node reads blocks from disk
	- but for frequently accessed files ***the blocks*** may be explicitly cached in the data node's off-heap block cache mem
	- by default, a block is cached in only one data node's memory, configurable on a per-file basis
	- users or applications instruct the name node on which files to cache(and for how long) by adding a cache directive to a cache pool
		- cache pools are an administrative grouping for managing cache permissions and resource usage

## HDfS federation
- for large clusters with many files, scaling might become difficult with a single namenode
- HDfS federation was introduced in Hadoop 2.x
	- allows a cluster to scale by adding namenodes
	- each of which manages a portion of the filesystem namespace
		- ex: one namenode manages files rooted under /user
- under federation, each nodenode manages
	- a namespace volume, which is made up of the metadata for the namespace
		- independent of each other, name nodes dont comms with each other
		- failure of one does not affect others
	- block pool containing all the blocks for the files in the namespace
		- block pool is a logical grouping of data blocks, managed independently by the cluster
		- blocks from one pool are managed independently of blocks from other pools, even though they may be stored on the same set of Data Nodes
		- block pool storage not partitioned
		- so data-nodes register with each namenode in the cluster and store blocks from multiple block pools

## HDfS high availability
- the combination of replicating namenode md on multiple fs and using secondary namenode to create checkpoints protects against data loss
	- but does not provide high availability of the filesystem
- namenode is collectively the single point of failure
	- whole hadoop system would be out of service until a new name-node could be brought online
	- a new primary namenode is started with one of fs metadata replicas
		- configure datanodes and clients to use this as the new namenode
	- new namenode cannot serve requests until
		- it has loaded its namespace image into memory
		- replayed its edit log
		- received enough block reports from the data nodes to leave safe mode
- until Hadoop 2, this restarting of name node or instantiating a new name node took so much time
	- this affected maintenance more
- Hadoop 2 introduced HDfS high availability implementation
	- this allows active-standby configuration: pair of namenodes
	- in the event of the failure of active, standby takes its place without significant interruption withing few tens of seconds
- this new implementation required architecture changes
	- the namenodes must use highly available shared storage to share the edit log
		- when a standby comes up, it uses the edit log to synchronize state with the active namenode
	- data nodes must send block reports to both namenodes, as the file-block mapping is stored in memory
	- clients must be configured to handle namenode failure, using a mech that is transparent to the users
		- clients mean, applications or tools that interacts with the HDfS
	- the secondary namenode's role is subsumed by the standby, which takes periodic checkpoints of the active namenode's namespace
- there are two options for the highly available shared storage: an NfS filer, or a quorum journal manager(QJM)(recommended)
	- QJM is a dedicated HDfS implementation
		- designed for providing highly available edit log
	- QJM runs as a group of journal nodes, and each edit must be written to a majority of journal nodes
	- typically there are 3 journal nodes, so the system can tolerate the loss of one of them
- if the active namenode fails, the standby can take it's place within 10s of seconds
	- as the latest state is in memory: both the latest edit log entries and an up-to-date block mapping
	- but the actual failover time observed will be longer in practice(around a minute or so)
		- because the system needs to be conservative in deciding that the active namenode has failed
- if standby is also down, from an operational point of view it is an improvement
	- because the process is a standard operational procedure built into hadoop
	- in both non-HA and HA with failed standby there is manual work involved in restarting the node. 
		- but in case of HA, the procedure is part of a well-defined, pre-configured fw
			- the process is structured and built into the hadoop's HA architecture
			- md recovery is not needed as active and standby would have shared the same md
			- no complex recovery procedure
		- in case of non-HA, recovery involves restarting NN, potentially running many commands
			- downtime is generally higher
			- manual checks required, validations or recovery steps
			- we will have to take care rebuilding the state

#### failover and fencing
- transition from an active to standby namenode is managed by a new entity in the system called failover controller
	- there are various failover controllers
	- but the default implementation uses Zookeeper
	- uses simple heartbeating mechanism
	- this controller is a process whose job is to monitor its namenode for failure and trigger a failover
	- looks like the controller runs on the same machine
	- one controller for one namenode
- failover may also be initiated manually by an administrator
	- called graceful failover
- in case of ungraceful failover, it is impossible to ensure that the failed namenode has stopped running
	- ex: slow nw, or a nw partition can trigger a failover transition
		- the namenode will continue to think it is the active one
- HA implementation goes to great lengths to preven previously inactive namenode from doing any damage
	- method called fencing
- there are fencing mechanisms
	- depending upon the shared edit log, the complexity increases
		- QJM only allows one namenode to write to the edit log at one time
			- it is still possible for the previously active namenode to serve stale read requests to clients
				- stale read-requests: provide file system metadata or data that might be outdated
			- so setting up an SSH fencing command that will kill namenode's process is a good idea
				- mechanism to isolate a problematic node from the rest of the cluster
				- involves using SSH to access the node and perform actions like rebooting, shutting down or disabling services
		- stronger fencing mechs are required while using Nfs filre for the shared edit log
			- since nfs filer accepts more than one writer to write at a time
			- fencing mech:
				- revoke access of namenode to the shared storage dir
				- diable it's nw port via a remote management command
				- STONITH: Shoot the other node in the head
					- specialized power distribution unit to forcibly shutdown the host
- client failover: client operations(like read/write) continue transparently even if the primary server fails
	- client requests are redirected to standby
	- can be handled in client side:
		- HDfS clients(hdfs dfs) use the HDfS client lib to interact with HDfS
		- handled by configuring host name to two namenode addresses in a hdfs-site.xml
		- client library tries each namenode address until operation succeeds

## The CLI
- fs.defaultFS: sets the default filesystem for Hadoop
	- by default HDfS
	- specified as a URI. ex: hdfs://localhost/
	- this is used to identify the host and port for the HDfS namemode
		- used by clients and HDfS daemons
	- in the above example, host is localhost and port is the default 8020
- dfs.replication
	- replication factor that decides how many datanodes does this block needs to be replicated in
- hadoop fs -help
- `hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/thirs/quangle.txt`
	- we could have omitted the scheme and host of the URI and picked up the default, hdfs://localhost as specified in core-site.xml
	- what does scheme mean?
		- scheme indicates how the resource should be accessed and identifies the protocol to be used such as http, https, ftp etc...
		- scheme here is hdfs
	- `hadoop fs -copyFromLocal input/docs/quangle.txt /user/thirs/quangle.txt`
	- we also could have used a relative path and copied the file to our home dir
		- `hadoop fs -copyFromLocal input/docs/quangle.txt /user/thirs/quangle.txt`
- hadoop fs -copyToLocal quangle.txt quangle.copy.txt
	- to check they are same: md5 input/docs/quangle.txt quangle.copy.txt
- hadoop fs -mkdir books
- hadoop fs -ls
	- similar to ls -l
	- first column: file permissions
	- second is replication factor of the file
		- 0 for directories and replication factor does not apply to dirs
		- directories are treated as metadata and stored by the name node, not the datanodes
	- 3rd and 4th: owner and the group
	- 5th: size of the file in bytes, 0 for directories
	- 6th and 7th are modified date and time
	- 8 the is the name
![[Pasted image 20241218122023.png]]
- HDfS has a permissions model for files and directories that is much like the posix model
- 3 types of permissions: r, w, x
	- r: to read files or list contents of a dir
	- w: to write a file or create or delete files and directories in a directory
	- x: ignored, as we cannot execute files on HDfS, and for a directory this permission is required to access its children
- by default hadoop runs with security disabled
	- client's identity is not authenticated
	- it is easy to impersonate, let's say there is a user x on hadoop. A client from a remote machine can have the same user name and login to hadoop and impersonate
	- but it is better to have permissions enabled to avoid accidental modification or deletion
- when permission checks are enabled, owner permissions are checked if the username matches the owner, group permissions are checked if the client is a member of the group, otherwise others permissions are checked 
- each file and directory has an owner, a group and a mode
- mode is made up of permissions for the users who is the owner, users who are members of the group, and the permissions for users who are neither the owner nor member of the group
- there is a concept of superuser, which is the identity of the namenode process
	- Permissions checks are not performed for the superuser

## Hadoop filesystems
- what is a filesystem?
	- a filesystem is a system that manages how data is stored, organized, retrieved and managed on a storage device
	- it acts as an abstraction layer between the operating system and physical storage
	- filesystems structure data in a hierarchy of files and directories to make it easier to locate and manage
	- provides mechanism to locate and retrieve data stored on the storage medium
	- ensure that the data is not corrupted and provide mechanisms to recover it in case of failures
	- enforce permissions and security rules
	- store metadata
- why do we need a filesystem?
	- without a fs, data would be stored as a raw sequence of bytes with no structure or labels, making it impossible to differentiate between files and data types
	- allow efficient allocation of storage space and retrieval of files
		- help avoid fragmentation and optimize disk usage
	- provide and abstraction for users to work with
	- data sharing via nwed filesystems
	- security
	- advanced filesystems target fault tolerance and backup
- hadoop provides many interfaces to its filesystems
	- hdfs is just one implementation
- hadoop ships with many such implementations
- hadoop can interact with different types of filesystems, not just HDfS
- hadoop is written in java, so most Hadoop filesystem interactions are mediated through the JAVA APi
	- by exposing its filesystem interface as a JAVA API, hadoop makes it harder for non-java apps to access HDfS
	- the HTTP rest API exposed by the WebHDfS protocol makes it easier for other languages to interact with HDfS
	- HTTP interface is slower than native java client, so should be avoided for very large data
	- there are two ways to access HDfS over HTTP: 
		- directly, where the HDfS daemons serve HTTP requests to clients
		- via a proxy, which accesses HDfS on the client's behalf using the usual Distributedfilsystem API
- Hadoop generally uses the URI scheme to pick the correct filesystem instance to communicate with

![[Pasted image 20241220131140.png]]
- in the first case, the embedded web servers in the namenode and datanodes act as WebHDfS endpoints
	- configurable, we can switch on or off this behavior
	- file md operations handled by namenode, while read(and write) operations are sent first to the namenode
		- which sends an HTTP redirrect to the client indicating the data node to stream file data from (or to)
		- after receiving the redirect, the client directly interacts with the specified datanodes to read or write
		- this design prevents namenode from becoming a bottleneck
- second way involves interacting with one or more proxy serverrs
	- these proxies are stateless, so they can behind a standard load balances
	- all traffic to the cluster passes through the proxy
	- client never accesses namenode or datanode
		- allows for stricter firewall and bandwidth limiting policies
			- how?
				- by centralizing access, sys admins can enforce security and traffic rules at a single point(proxy servers), rather than having to manage across datanodes and namenodes
				- attack surface is limited by banning connections to datanodes and namenodes
					- clusters can have strict firewall rules allowing only connections to/from proxies
				- rate limiting can be applied to the proxy layer
	- common to have proxies for 
		- transfer between hadoop clusters located in different data centers,
		- when accesing a Hadoop cluster running in the cloud from an external nw
- HTTPfs proxy exposes same HTTP (and HTTPS) inteface as webHDfS
	- HTTPfS and WebHDfS both enable HTTP/HTTPs based access to HDfS, but they serve diff purposes and have distinct diff
	- WebHDfS: Native built in rest api provided by HDfS to directly access HDfS over HTTP
		- server runs on each namenode and datanode
	- HTTPfS:
		- proxy server for HDfS that provides an HTTP i/f to access HDfS
		- but routes all requests through a central proxy
		- main diff is this is a server run by a specific script
- Hadoop also provides C lib that mirrors the one in Java
	- it can be used to access any Hadoop filesystem
	- works using the Java Native Interface to call a java filesystem client
		- similar to py4j but for C
- Hadoop can run with multiple filesystem implementations at the same time
1412