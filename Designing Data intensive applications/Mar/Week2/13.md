## Star-schema
- txn processing systems have a wide range of data models
- analytical processing systems ***do not have as many options***
- much common one is dimensional model which results in a star schema
- i have skipped much of the content
- ftr = event
- dt = who, what, when, where, how and why
- a ***variation*** of star schema is snowflake schema
	- where dims are broken down into ***subdims***
	- ***normalized star schemas***

## Column-Oriented Storage
- in most OLTP dbs, storage is laid out in a row-oriented fashion
	- all the values from one row of a table are stored next to each other
	- ddbs are similar
- column-oriented storage:
	- store all the values from each column together
	- each col=separate file
	- applies equally to relational and non-relational data models(ex: docs)
	- relies on each different col's file containing the rows in the same order

### Col Compression
- col-oriented storage(COS) lends very well to ***compression***
- compression reduces the ***demand*** on disk throughput
- repetitive values are a good sign for compression
- ***depending on data*** in the col, different compression techs can be used
	- ex: bitmap encoding
- take a col with n distinct values and turn it into n separate bitmaps
	- ***one bitmap for each distinct value***
	- one bit for each row(1 or 0)
- if n is bigger, there will be more zeros in each bit map
	- 1s are ***sparse***
	- in this case, bms can be run-length encoded
	- this makes the repr compact
![[Pasted image 20240313200316.png]]

![[Pasted image 20240313200611.png]]
- there are other compression schemes for different kinds of data, not gone through in the book
- BigTable model is mostly-row oriented

#### Mem bw and vectorized processing
- for DWH queries, a big bottleneck is ***bw*** for getting data from ***disk to mem***
	- not the only bottleneck
		- bw from main mem to CPU Cache
			- called ***memory bw***
				- measure of how fast the CPU can access main mem
			- avoiding [branch misprediction](https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array?lq=1) and bubbles in the CPU instruction processing pp can help us efficiently use this mbw
			- using [SIMD]([Basics of SIMD Programming (cvut.cz)](http://ftp.cvut.cz/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html#:~:text=SIMD%20is%20short%20for%20Single,data%20is%20called%20scalar%20operations.)) instructions in modern CPU makes processing efficient
				- SIMD is a ***type of parallel processing*** that enables processing of multiple data with a single instruction
				- Single Instruction, Multiple Data
- CPU cycle: basic unit that measures a CPU's speed
	- during ***each cycle***, billions of transistors within the processor open and close
	- this is how CPU executes the calculations contained in the instructions
- COS also efficient for making efficient use of ***CPU cycles***
	- ex: query engine can take a chunk of compressed col data that fits comfortably in CPU's L1 cache and iterate through it in a tight loop(with no fn calls)
	- CPU can execute such a loop much faster than code that requires a lot of fn calls and conditions for each record
- column compression allows more rows from a col to fit in the same amount of cache
- the bitwise AND and OR can be designed to operate on compressed cols
	- this tech is called vectorized processing