- if all keys and values had a ***fixed size***, we could use ***binary search on a segment file*** and avoid in-mem index
	- the problem with variable length in practice is that it is difficult to tell where one record ends
	- if the index is sparse, then how will we know where the records start and end for keys that do not have an entry in index? ^^^^
### Constructing and maintaining SSTables
- SST addresses the cons of the log based storage in [[29]]
	- indices could be sparse, thus fitting in RAM is easier
	- Range queries are made possible
- how to store data sorted by key in the first place?
	- as ***incoming writes*** occur in ***any order***
- maintaining sorted structure in disk is possible but ***maintaining it in mem is much easier***
- there are plenty of ***tree ds*** that facilitate insert at any order and read them back in sorted order:
	- AVL trees, red-black trees
- storage engine can be made to work like the following:
	- when a write comes in, ***add entry to an in-memory balanced tree ds*** 
		- this in-mem tree is sometimes called ***memtable***
		- what happens if there is write request for a key that is already part of the tree? will the value be over-written in mem? ^^^^
	- once the memtable ***exceeds a size threshold***, ***write*** it out to disk as a SST file
		- this can be done because the tree already maintains the kvp sorted by key
		- the ***new SST file*** is the ***most recent segment*** of the db
		- while the SST file is written to disk, ***new write req*** can continue to a ***new memtable instance***
	- for a read request, ***try*** to find the key ***in the mem table***
		- if not found, search in the most recent segment file in disk and so on
	- merging compaction mandatory
- obvious ***con*** of the above approach is ***vulnerability to system crash***
	- before the write of the tree to disk, if the system crashes, we will ***loose most recent writes***
	- one soln might be to have a ***log in disk***(similar to log based storage)
		- this does ***not*** store ***sorted records***
			- ***main purpose*** is to ***restore memtable*** after a crash
		- the log can be ***discarded*** as soon as the memtable is written to SST

### Making an LSM-tree out of SST
- LSMT and SST are not indexes
	- they are storage techniques
- the algorithm above is used in some kv storage engines
	- terms ***SSTable*** and ***memtable*** were introduced in the Google BT paper
- storage engines that are based on the ***principle of merging and compacting sorted files*** are often called ***LSM storage engines***
	- LSM = Log-structured Merge-Tree
- what is the difference between SST and LSMT?
	- SST is a format of storing kvp sorted by key in disk
	- LSMT is a layered ds, involving two or more separate structures(in our case mem and disk) each of which is optimized for its storage medium
		- LSMT allows SSTables to exist without the controversy of being both sorted and append-only
- a full-text index implementation:
	- implemented using kv structure
		- where key is a word(a ***term***) and the value is a list of IDs of all the documents that contain the word
			- here documents can mean web pages, product desc etc..
		- this mapping is kept in SST-like sorted files, which are merged in the bg as needed
			- unique values are added to the list instead of being discarded based on recency
		- no mention of what will be in mem
			- will the key be assigned to a list of new values without the inclusion of old values? ^^^^
		- will LSMT storage engines be supported with HMI for each sorted segment file?

### Performance optimizations
- the ***LSM-tree algos slow*** in looking up ***non-existent keys*** in the db
	- we might have to ***check till the oldest segment*** so that we are sure that the key does not exist
		- this involves disk io
	- in order to ***optimize*** the above kind of access, storage engines often use additional ***bloom filters***
	- Bloom filters: ***mem efficient ds*** for ***approximating*** the ***contents*** of a set
		- it can tell if a key does not exist in the db, thus saves unnecessary disk reads
- there are different strategies to determine the ***order and timing of how SSTables are compacted and merged***
	- most common options are size-tiered and leveled compaction
	- Cassandra supports both
	- In size-tiered compaction, newer and smaller SSTables are successively merged into older and larger SST
	- in leveled compaction, the key range is split up into smaller SSTs and older data is moved into separate levels which allows compaction to proceed more incrementally and use less disk space
- range queries are efficient as we have sorted data
- high write throughput support because the disk writes are sequential
	- because sequential writes are performant than the random writes