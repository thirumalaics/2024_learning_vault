## SSTables and LSM-Trees
![[Pasted image 20240302162052.png]]
- above, the kvp in the log occur in the order they were written, and values later in the log take precedence for new/old key
	- other than this aspect, ***order is inconsequential***
- a ***change*** to the format of our ***segment file***: ***sequence of kvp is sorted by key***
	- this format can be called as ***sorted string table***(SST)
- we also require that each key only appears once within each merged segment file
	- compaction ensures that
- ***SST*** has ***several big advantages*** over log segments(LSMT) with hash indexes
	- merging segments is ***simple and efficient***, even if the files are bigger than the available memory ^^^^
		- how is the following approach easier? does it not involve opening each sf side by side?
			- I think this is because the segments are sorted already
			- in LSMT, we need to look through files starting from the recent to old and get the unique keys
			- in SST, we just need to compare the segment heads on each iteration and choose the lowest one
		- remember that each segment is sorted inherently even before merge and compaction
		- merge approach is similar to ***mergesort*** algo
			- we start reading the input files side by side
			- look at the first key in each file, copy the lowest key(according to the sort order) to the op file and repeat
		- the above approach produces ***a new merged segment file***, also sorted by key
		- when multiple segments contain the same key, we can ***keep the value from the most recent segment*** and discard the values in older segments
		- ![[Pasted image 20240303180900.png]]
	- in order to find a particular key in the file, ***no need*** to keep an ***index of all the keys*** in memory
		- we ***still need an in-mem index***, but the distribution of keys can be ***sparse***
		- ***one key for every few kilobytes*** of segment file is sufficient
			- because few kbs can be scanned quickly
		- this is made possible because of the sorting
			- ex:
			-  let's say we are looking for a key `apple` and we know offsets for keys `age` and `array`, now we can jump to offset of age and start scanning until we find apple
				- why because apple is bw age and array as per sort order
		- ![[Pasted image 20240303182407.png]]
	- since read requests need to scan over several kvp in the requested range anyway, we can ***group the records into a block and compress it b4 writing to disk***
		- this compression indicated by the grey block in 3.5
		- each entry of the ***sparse in-mem index***, ***points*** at the ***start of a compressed block***
		- this approach saves space and reduces IO bandwidth use