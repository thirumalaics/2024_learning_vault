why was character set needed in the first place?
- earlier, the only characters that mattered were unaccented english letters
- ascii was able to represent every character using a number between 32 and 127
	- this could be conveniently in 7 bits
	- most computers in those days were using 8-bit bytes, so not only could we store every possible ASCII char, but we had a whole bit to spare
	- in some word processors, the high bit was turned on the high bit to indicate the last letter in a word
	- codes below 32 were called unprintable as they were used for control characters
		- 7 made computer beep
- the extra one high bit allowed many to come up with their own use for the possible 128 characters that it provided
	- the IBM PC had something that came to be known as the OEM char set
		- it provided some accented characters for european languages and a bunch of line drawings
	- outside USA, all kinds of OEM character sets were dreamed up, which all used the top 128 chars for their own purpose
		- For example on some PCs the character code 130 would display as é, but on computers sold in Israel it was the Hebrew letter Gimel (![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)), so when Americans would send their résumés to Israel they would arrive as r![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)sum![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)s
- eventually, this OEM free for all got codified in the ANSI std
	- everyone agreed on what to do below 128, which was pretty much the same as ASCII
	- but there were lot of different ways to handle the chars from 128 and on up depending upon where one lived
	- these different systems were called code pages
		- israel DOS used a code page called 862, while greek users used 737
	- in asian languages, which had more than 1000 letters - 8 bits was not enough
		- the solution was DBCS: double byte character set
			- each character is either 1 or 2 bytes
			- the first of a two byte character comes from a specific range called a lead byte range, utilizes all 8 bits
			- the second by has its own range, starts with 7 bits and may utilize all 8 bits
			- in case of single byte, only 7 bits are utilized
		- some letters were stored in one byte and others took two
		- it was easy to move forward in a string, impossible backwards
			- moving forwards, if we land on a lead byte(utilizes 8 bits), we know we are at the start of a 2-byte character - read the next byte and move forward 2 bytes total
			- while moving backwards, since the trail byte can fall within a lower range there is not an easy way to identify if it is part of a 2 byte string
		- programmers were encouraged not to use s++, s-- 
		- alternate way to navigate through a string was: AnsiNext and AnsiPrev
- 
- what was the need for ASCII? 
	- Computers understand 0's and 1's. Different types of data need to be represented using these 0's and 1's so that we can store and process data. ASCII is one way/mapping to represent English characters in terms of 0s and 1s.
	- ASCII provides consistency across systems
	- networks, keyboards, printers all needed a shared format
	- ascii also is compact for english and common symbols with 7 bits
	- ascii also included control codes(like newline, tab) for formatting and device instructions
- Ascii provided mapping for 7 bits and english characters. The remaining one bit of the byte was used to represent different characters in different computers based on region.
- ASCII collapsed when the internet came into picture, connecting computers across the world. 
- So when strings were transferred from one computer to another, the content was deciphered differently depending on the region.
- what is unicode? 
	- effort to create a single character set that included every reasonable writing system on the planet
	- unicode is not a 16bit code where each char takes 16 bits and therefor 65536 characters
	- until now, our assumption is that a letter maps to some bits which we can store on disk or in memory
	- in unicode a letter maps to something called a code point which is still just a theoretical concept
	- how that code point is represented in memory or on disk is another story
- every letter in every alphabet is assigned a magic number by the unicode consortium
	- U+0639 -> code point
	- U+ means unicode and the numbers are hexadecimal
	- there is no real limit on the number of letters that unicode can define and in fact they have gone beyond 65536
	- Hello : U+0048 U+0065 U+006C U+006C U+006F.
- to represent unicode code points in memory, we need to understand encoding
	- initial ideas for unicode encoding was that, let's store those numbers in two bytes each, so Hello becomes: 
		- 00 48 00 65 00 6C 00 6C 00 6F
		- but it could also be: 
			- 48 00 65 00 6C 00 6C 00 6F 00
	- endianness(byte order):
		- refers to how multi-byte values are stored in memory
		- Big-endian: most significant ***byte*** first
		- little-endian: least significant ***byte*** first
		- different CPUs prefer different orders
	- early implementors of unicode wanted to be able to store their unicode code points in high-endian or low endian mode, whichever their particular CPU was fast at
	- if we read a big-endian file using little-endian reader, the bytes `fe ff` appear reversed as 
	- BOM: Byte order Mark: special marker at the beginning of a file or data stream
	- Let's say the content of my datastream represented in hex(big-endian) as: af fa ff 00. I am sending this content over to another system where little-endian is used. Without BOM, the content is recognized in little-endian to be: af fa ff 00 instead of fa af 00 ff. In order to avoid this misunderstanding, BOM is used at the start of the data stream like a header, so in my example this header is fe ff(this text is big endian), so it helps the recipient to read by data stream as it is intended to be in little endian: fa af 00 ff.
	- the problem with byte order starts with UTf-16, where there are multiple bytes
		- but some editors include a BOM to signal this file is UTf-8
		- UTf BOM: ef bb bf
	- the problem is made worse as not every unicode string in the wild has a byte order mark at the beginning
	- this unicode 16 version had it's fair share of backlash from english programmers, who hade no use of the extra byte
		- also all the existing documents were in ANSI(7bit/1 byte) and DBCS char sets, so someone had to convert all these
		- for this reason alone most people decided to ignore Unicode for several years
	- this brought about invention of UTf-8
		- another system for storing our string of unicode code point in memory using 8 bit bytes
		- every code point from 0-127 is stored in a single byte
		- only code points 128 and above are stored using 2,3, or even upto 6
		- neat side effect of representing english text exactly the same
		- other language users had to jump through loops, as in, they will have to use several bytes to store a single code point

1825

https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/