why was character set needed in the first place?
- earlier, the only characters that mattered were unaccented english letters
- ascii was able to represent every character using a number between 32 and 127
	- this could be conveniently in 7 bits
	- most computers in those days were using 8-bit bytes, so not only could we store every possible ASCII char, but we had a whole bit to spare
	- in some word processors, the high bit was turned on the high bit to indicate the last letter in a word
	- codes below 32 were called unprintable as they were used for control characters
		- 7 made computer beep
- the extra one high bit allowed many to come up with their own use for the possible 128 characters that it provided
	- the IBM PC had something that came to be known as the OEM char set
		- it provided some accented characters for european languages and a bunch of line drawings
	- outside USA, all kinds of OEM character sets were dreamed up, which all used the top 128 chars for their own purpose
		- For example on some PCs the character code 130 would display as é, but on computers sold in Israel it was the Hebrew letter Gimel (![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)), so when Americans would send their résumés to Israel they would arrive as r![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)sum![ג](https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2003/10/gimel.png?resize=5%2C9&ssl=1)s
- eventually, this OEM free for all got codified in the ANSI std
	- everyone agreed on what to do below 128, which was pretty much the same as ASCII
	- but there were lot of different ways to handle the chars from 128 and on up depending upon where one lived
	- these different systems were called code pages
		- israel DOS used a code page called 862, while greek users used 737
	- in asian languages, which had more than 1000 letters - 8 bits was not enough
		- the solution was DBCS: double byte character set
			- each character is either 1 or 2 bytes
			- the first of a two byte character comes from a specific range called a lead byte range, utilizes all 8 bits
			- the second by has its own range, starts with 7 bits and may utilize all 8 bits
			- in case of single byte, only 7 bits are utilized
		- some letters were stored in one byte and others took two
		- it was easy to move forward in a string, impossible backwards
			- moving forwards, if we land on a lead byte(utilizes 8 bits), we know we are at the start of a 2-byte character - read the next byte and move forward 2 bytes total
			- while moving backwards, since the trail byte can fall within a lower range there is not an easy way to identify if it is part of a 2 byte string
		- programmers were encouraged not to use s++, s-- 
		- alternate way to navigate through a string was: AnsiNext and AnsiPrev
- 
- what was the need for ASCII? 
	- Computers understand 0's and 1's. Different types of data need to be represented using these 0's and 1's so that we can store and process data. ASCII is one way/mapping to represent English characters in terms of 0s and 1s.
	- ASCII provides consistency across systems
	- networks, keyboards, printers all needed a shared format
	- ascii also is compact for english and common symbols with 7 bits
	- ascii also included control codes(like newline, tab) for formatting and device instructions
- Ascii provided mapping for 7 bits and english characters. The remaining one bit of the byte was used to represent different characters in different computers based on region.
- ASCII collapsed when the internet came into picture, connecting computers across the world. 
- So when strings were transferred from one computer to another, the content was deciphered differently depending on the region.
- what is unicode? 
	- effort to create a single character set that included every reasonable writing system on the planet
	- unicode is not a 16bit code where each char takes 16 bits and therefor 65536 characters
	- until now, our assumption is that a letter maps to some bits which we can store on disk or in memory
	- in unicode a letter maps to something called a code point which is still just a theoretical concept
	- how that code point is represented in memory or on disk is another story
- every letter in every alphabet is assigned a magic number by the unicode consortium
	- U+0639 -> code point
	- U+ means unicode and the numbers are hexadecimal
	- there is no real limit on the number of letters that unicode can define and in fact they have gone beyond 65536
	- Hello : U+0048 U+0065 U+006C U+006C U+006F.
- to represent unicode code points in memory, we need to understand ***encoding***
	- initial ideas for unicode encoding was that, let's store those numbers in two bytes each, so Hello becomes: 
		- 00 48 00 65 00 6C 00 6C 00 6F
		- but it could also be: 
			- 48 00 65 00 6C 00 6C 00 6F 00
	- endianness(byte order):
		- refers to how multi-byte values are stored in memory
		- Big-endian: most significant ***byte*** first
		- little-endian: least significant ***byte*** first
		- different CPUs prefer different orders
	- early implementors of unicode wanted to be able to store their unicode code points in high-endian or low endian mode, whichever their particular CPU was fast at
	- if we read a big-endian file using little-endian reader, the bytes `fe ff` appear reversed as 
	- BOM: Byte order Mark: special marker at the beginning of a file or data stream
	- Let's say the content of my datastream represented in hex(big-endian) as: af fa ff 00. I am sending this content over to another system where little-endian is used. Without BOM, the content is recognized in little-endian to be: af fa ff 00 instead of fa af 00 ff. In order to avoid this misunderstanding, BOM is used at the start of the data stream like a header, so in my example this header is fe ff(this text is big endian), so it helps the recipient to read by data stream as it is intended to be in little endian: fa af 00 ff.
	- the problem with byte order starts with UTf-16, where there are multiple bytes
		- but some editors include a BOM to signal this file is UTf-8
		- UTf BOM: ef bb bf
	- the problem is made worse as not every unicode string in the wild has a byte order mark at the beginning
	- this unicode 16 version had it's fair share of backlash from english programmers, who hade no use of the extra byte
		- also all the existing documents were in ANSI(7bit/1 byte) and DBCS char sets, so someone had to convert all these
		- for this reason alone most people decided to ignore Unicode for several years
	- this brought about invention of UTf-8
		- another system for storing our string of unicode code point in memory using 8 bit bytes
		- every code point from 0-127 is stored in a single byte
		- only code points 128 and above are stored using 2,3, or even upto 6
		- neat side effect of representing english text exactly the same
		- other language users had to jump through loops, as in, they will have to use several bytes to store a single code point
	- UCS-2 stands for Universal character set coded in 2 octets, it is a character encoding standard
		- encodes characters using exactly 2 bytes(16 bits) per character
		- can only represent characters in Basic multilingual place(BMP):
			- handful of languages
		- it cannot represent characters outside the BMP: such as emoji or many rare chinese chars
		- UTf-16 is a superset of UCS-2
	- difference between utf-16 and ucs-2
		- utf-16 uses 2 bytes for characters in the BMP
		- it also supports characters outside the bmp by using surrogate pairs(4 bytes total)
			- utf-16 is a variable length encoding
			- still subject to endianness like UCS-2
		- UCS-2 does not support surrogate pairs, so it's not compatible with newer unicode characters beyond U+ffff
	- there are a bunch of other ways of encoding Unicode
		- there is UTf-7, which is a lot like utf-8 but guarantees that the high bit will always be zero
		- there is UCS-4, which stores each code point in 4 bytes
			- every single code point can be stored in the same number 
	- unicode code points can be encoded in any old-school encoding scheme, too
		- we can encode in ASCII or OEM etc..
		- but with one catch that some of the letters might not show up if there is no equivalent for the unicode code point we are trying to represent in the encoding
		- we get `?` or a box
		- there are hundreds of traditional encodings which can only store some code points correctly and change all the other code points into `?`
	- base64
		- used to convert any binary data into a string of printable ascii characters
		- it does this by taking groups of bits from the input and mapping them to a specific set of 64 characters(A-Z, a-z, 0-9, +,/,= used at the end if the input does not perfectly align)
		- each of the characters mentioned above represent a unique 6-bit value(64chars)
		- Man: 01001101 01100001 01101110 -> 010011(19) 010110(22) 000101(5) 101110(46)
			- now look up respective chars: TWFu
		- base64 works with groups of 24 bits which nicely divides into 4 base64 chars (4 chars * 6 bits/characters)
		- if our input data is a multiple of 3 bytes, we don't need padding
		- if we have 1 or 2 bytes left over at the end, padding is used
		- Ma: 010011 010110 0001 -> 010011 010110 000100(00 added at the end to make it 6) -> TWE -> TWE=
		- M: TQ==
	- how can utf-7 can store any unicode code point correctly?
		- inefficient and deprecated
		- serious limitations
		- designed for systems that only support 7 bit data
		- was invented as a workaround - to allow full unicode text to be represented safely over 7-bit channels
		- does it through a special encoding trick
		- divides characters into three main classes
			- directly encodable characters
				- safe ascii characters and are encoded as is
			- optional direct characters
				- ascii chars that could be encoded directly or encoded in base64
				- may vary between strict and lenient UTf- 7 implementations
			- unicode-only characters:
				- any character outside 7-bit ascii set is not directly encodable and must be encoded using base64 in a shifted sequence
		- example of unicode-only characters:
			- ✓ has unicode U+2713, decimal: 10003, bin: 00100111 00010011
			- base64: 001001 110001 001100 -> JxM=
				- this JxM is decoded not assuming ascii, base64 has it's own mapping between numbers and text
				- Content-Transfer-Encoding: base64
				- Content-Type: text/plain; charset=utf-8
					- these help the receiving app to interpret these bytes back into human readable characters
				- how does the decoder in recipient side knows how many zeros were appended?
					- any unicode code point is represented with minimum multiple of 8 bits needed, even if the unicode code point does not turn on all the 8 bits of a byte. 
					- So when we 
	- when transmitting data, we also provide Content-Transfer-Encoding: base64 header
	- https://chatgpt.com/c/687cfd0a-5d40-8011-8642-4cbf3d4d63da
https://chatgpt.com/c/687cfd0a-5d40-8011-8642-4cbf3d4d63da
2012

https://gemini.google.com/app/616cc672b620a27c -> this is for base 64 
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/


1540 -> 26