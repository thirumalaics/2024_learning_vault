- two families of storage engines: ***log-structured***, and ***page-oriented*** storage engines such as [[05#B-Trees]]

## Data Structuress that power our DB
![[Pasted image 20240228183205.png]]
- these two fns implement a kv store
- similar to what db_set does here, many dbs internally use a ***log***, which is an ***append-only data file***
	- db_set is efficient
	- appends are efficient
- real dbs have more issues to deal with
	- concurrency control
	- reclaiming disk space so that the log does not grow forever
		- a combination of table optimization, log management, archiving old data, and cleaning up unnecessary database objects
	- handling errors
	- partially written records
-  ***log*** referred through out the book is of general sense:
	- an append-only sequence of records
	- it does not have to be human-readable
	- might be just binary
- the db_get is ***not efficient***: O(n)
	- because it has to go through the entire file even to match a single key
- to make it efficient we need a different ds: ***an index***
	- range of indexing structures are discussed
	- the general idea behind index is to keep some ***additional md*** on the side which  acts as a signpost when we search
	- to ***search*** the ***same data in several ways***, we may need ***different indexes*** on ***different parts of the data***
- an index is an ***additional structure*** that is ***derived*** from the primary data
	- dbs allow us to add and remove indexes
	- this does not affect the contents of the db
	- affects the query performance
	- ***any kind of index*** slows down write
		- because index also needs to be updated every time data is written
	- indexes incur overhead
	- for this reason only dbs do not index everything by default
	- more indexes, more overhead

### Hash indexes
- indexes for kv data are seen below
	- very common data to be indexed
	- the key in terms of RDB can be PK, value can be Record/tuple
- ***kv*** stores similar to the ***dict*** type in programming langs(pl)
	- in pl, ***implemented*** ***as hash map***
- there are hash maps for our in-mem DSs, why not use them to index data on disk?
- default storage engine in ***a db*** does the following:
	- keep an in-mem hash map where every ***key*** is ***mapped*** to a ***byte offset in the data file*** - the location at which the value can be found
	- the hm is updated in case of append or update
	- storing the ***byte offset gets*** us the value with just ***one seek operation***
	- in this type of storage, the seek operation is the bottleneck
	- this method applies only if the ***keys fit in the available RAM***
	- ![[Pasted image 20240228191000.png]]
	- values can be bigger than the available mem, hence they can be loaded when required
	- if ***cache has the value, IO skipped***
		- cache is not same as index
	- well suited where the value for each key is updated frequently and num of distinct keys are less
	- if this storage engine is append only, we might run out of disk space
		- one good solution might be to break the log into ***segments of certain size***
		- this can be done by ***closing*** a segment of file when it reaches a certain size, making subsequent writes to a ***new segment file***
		- segmented files allows one logical file to occupy multiple physical files %%%%
			- allows data files to exceed the ***physical file size limit set by the OS***
			- segment files are a way to represent multiple physical files as one logical file
		- then ***compaction*** can be performed, which keeps the most recent value of a key after ***discarding*** ***duplicates***
![[Pasted image 20240228190921.png]]