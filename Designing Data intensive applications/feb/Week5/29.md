### Hash Indexing Contd
- compaction often ***makes each segment much smaller***
	- assuming keys are overwritten several times within one segment
![[Pasted image 20240302162052.png]]
- compaction and merging(segment files) can be done simultaneously
- ***merging happens across files***
- ***compaction occurs within a file***
- while the merging and compaction are going on in the background thread, we can ***serve read and write requests using the old Sfs***
- after merging:
	- read requests can use the new segment file 
	- old one can be deleted
- ***each segment has it's own in-mem Hash Table***
- remember there will be only one most recent segment where writing will happen
- in order to find value for a key %%%% ^afe100
	- we check the most recent segment's Hash Table
	- if not found in the above, check the next most recent segment's HT and so on
- the ***merging*** process ***keeps the number of segments small***
	- so that we do not need to check many hash maps
- for the above implementation to work there are many considerations and issues:
	- file format:
		- ***csv is not best format for log***
		- best to use a binary format that first encodes the length of a string in bytes, followed by the raw string(without need for escaping)
			- what is ***raw string***?
				- special type of string
				- allows us to include \\(backslashes) without interpreting them as escape sequences
			- fast and simple
		- in binary file, each data(char, num etc) occupies the ***same number of bytes on disks as it occupies in memory***
		- this is not the case with text-based files, where ***each piece of data is represented with a byte***
		- ex: 32667 takes 5 bytes in text file(one for each num), where as in binary file it takes 2bytes(same as in memory)
		- ***binary file = compact storage*** whereas ***text file = user-friendly storage***
	- deleting records
		- to delete a kvp, append a special deletion record to the data file
			- sometimes called the ***tombstone***
		- tombstone indicates, ***during the merging process***, to discard the key and it's values
	- crash recovery
		- in case of system restarts, the in-mem hash maps are lost
		- we can ***restore*** each segment's HM via ***brute force*** or ***store a snapshot of each segment's hash map on disk*** which can be loaded into mem quickly
	- partially written records
		- cases when the system crashes at halfway through an operation
		- use ***checksums***, which can be compared after restart to ignore corrupted parts
		- this means we have to store a checksum already
	- concurrency control
		- as records are written sequentially, common implementation choice is to have ***only one writer thread***
		- data file segments are append-only and otherwise immutable
			- they can cater to ***concurrent read requests***
- an append only design turns out to be good for several reasons
	- appending and segment merging are ***sequential write operations***
		- ***sequential operations are much faster in general than the random writes***
			- in both HDD and SSD
	- concurrency and crash recovery are much ***simpler*** if segment files are append only and immutable
		- ex: we do not have to worry about crash while overwrite, which leaves us with a file containing part of the old and new value spliced together
			- also writes are isolated when only one write thread is allowed, so there cannot be multiple users writing at the same time
			- read concurrency is handled because the HT holds the offset to the value that exists at the time being
				- only after the new value is written successfully in a new line, the offset changes
	- merging old segments avoids the problem of data files getting fragmented over time
		- file fragmentation occurs when a file is stored in non-contiguous sectors on a hard drive %%%%
			- causing it to take longer to open and decrease overall performance
		- https://en.wikipedia.org/wiki/Fragmentation_(computing)#Data_fragmentation:~:text=of%20size%200x4000.-,Data%20fragmentation,-%5Bedit%5D
- HTI limitations
	- ***hash table must fit in mem***
		- on-disk hm is possible but not efficient
			- requires a lot of random access IO
				- because key-searches are not sequential, but kv writes are sequential
			- expensive to grow when it becomes full
	- ***range queries are not efficient***
		- lookup for a range requires us to look up each key individually in HM
- so far we have seen Log structured Merge tree