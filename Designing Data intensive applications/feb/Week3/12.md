- approach 1 will cause struggle because we deferred the processing to be done during read and the number of read requests is huge
	- avg rate of published tweets << rate of timeline reads
	- so it is better to do more work at write time and less at read time
- approach 2 requires a lot of extra work right at the moment
	- this is much harder in cases where the followers amount up to millions
		- it means millions of write into caches as soon as a tweet is posted
- to describe load in the twitter use case: distribution of followers per user (maybe weighted by how often those users tweet)
- currently twitter uses a hybrid approach
	- tweets from people, with huge num of followers, are exempted from approach 2
	- these tweets follow approach 1, in which tweets are written to a pool and then they are fetched upon read

## Describing performance
- investigate what happens when load increases
	- increase a load parameter and keep the system resources(CPU, mem, nw) unchanged to see the effect on performance
	- find out the what increase in resources is required to keep the performance unchanged when we increase a load parameter
- both of the above questions require us to describe performance numerically, which allow us to understand what is the norm
- in ***batch processing***, we care about ***2 metrics*** to describe performance
	- ***throughput*** - number of records processed per second
	- ***total time*** it takes to run a job on a dataset of certain size
		- can this metric not be derived from the above?
			- because throughput can be deceiving in the real world
			- ideally, time taken should be (data size/throughput)
				- but due to ***skews or other reason***, the actual time might be more
- in online systems, ***response time*** is important
	- time between client sending a request and receiving a response
	- this includes time to process the request(service time) + nw delays + queueing delays 
- ***latency != response time***
	- latency is the ***duration*** that ***a request is waiting to be handled*** - during which it is latent(awaiting service)
	- so in a way can we say latency is part of response time?
		- i think yes
- the same request might not get same response time every time
- response time definitely varies for a system that handles a variety of requests
- response time should be thought of as a distribution of values that we can measure
- many factors affect ***response* *time***, many of which are random
	- one such factor might be the data that we send
- average response time is commonly used metric
	- average does not tell how many users actually experienced that delay
- better to use percentiles
	- ex: median aka 50th percentile(p50)
		- sort the distribution and choose the halfway point
	- a median response time let's us know that half of our requests return in less than that value and half of the requests take longer than that
