- median refers to a single request
- 95th percentile means 95 out of 100 requests take less response time than the current value
	- inversely, 5/100 take more than the current value
- high percentiles of response times, aka tail latencies, are important
	- because they also affect users' experience of the service
	- again, latencies and response times are not same but they are used synonymously
	- it is important to decrease response times in these percentiles
- percentiles are often used in SL-Objectives and SLAs
	- contracts that define expected performance and availability of a service
	- ex: an SLA may state that the service is up only when the median latency is less than 200ms and 99th percentile under 1s
		- if the response time is longer, the service is considered to be unavailable
- ***queuing delays*** often account for a large part of the response time at high percentiles
	- queuing delay: the amount of time spent by a request waiting in a queue to be processed
	- because servers can only process some number of requests in parallel(***limited not just by the usual CPU cores***)
	- it only takes a small number of slow requests to hold up processing of subsequent request
		- aka head-of-line-blocking effect
		- slow requests?
			- requests that are taking more time to get processed
- response times are measured in the client side
- keep in mind to test in such a way that artificial load generating clients ***keep sending the requests independently of the response time***
	- because we have to emulate the queuing time seen in reality
	- one of the factors affecting queueing time is ***input rate***
- ***high percentiles*** become specifically ***important*** in ***backend services***
	- these are called multiple times even for servicing ***a single request***
	- even if the calls are made parallel, the end-user-req needs to wait for the slowest of the parallel calls to complete
	- if an end-user request requires multiple backend calls, even if only small % of backend calls are slow, the chance of getting a slow call increases 
		- so higher proportion of end-user reqs end up being slow
		- aka tail latency amplification
			- a small percentage of high latency affecting the response time of many requests
	- ***call time*** means the ***max time taken out of all parallel calls***
	- decrease slow backend call to make the call time less
- adding response time percentiles to monitoring dashboards for our services require us to stream process metrics
	- ex: rolling window of response times of reqs in the last 10 mins

## Approaches for coping with load
- load params increase by some amount, how to cope?
- for fast growing service, it is likely that we will need to rethink our arch on every order of magnitude increase in load
- scaling out usually means distributing load across multiple smaller machines
- distributing load across multiple machines is aka shared-nothing arch
- higher end machines are costlier
- intensive wl cannot avoid scaling out
- ***good arch usually involve both scaling up and out***
	- using a fair amount of capable machines(not small)
- ***systems that can scale on their own***, without any human intervention, are called ***elastic***
	- for unpredictable loads
- distributing stateless wl is straightforward
- ***distributing stateful adds additional complexity***++
	- for this reason only it was common to keep db on a single node (scale up) until scaling cost or high-availability requirements force us to distribute
	- how is db a stateful system?
		- is it because it has to store state when performing joins or aggs
- architecture of systems that operate at large scale is usually highly specific to that app
- the problems with these systems might be the following or a mix of them:
	- volume of read, writes, data to store
	- complexity of the data, response time requirements, access patterns
- following is important to understand
- ex: a sys that is designed to handle 100,000 requests per second, each 1kb in size is not the same as
	- a system designed for 3 requests per minute, each 2 GB in size
	- both systems have the same throughput though
- an architecture that scales well for an app, is built around assumptions of which operations are common and which are rare - load parameters
	- imp to get the assumptions right
	- even though each arch is specific, they have common building blocks and patterns