## SW Errors
- hw faults are thought to be random and independent of each other
- one hw fault does not strictly mean the other hw is going to fail
	- weak correlation
		- ex: heating of server rack
	- hence unlikely that a large num of hw components will fail at the same time
- another class of fault is ***systematic error*** within the sys
	- harder to anticipate
	- because they are correlated across nodes, they cause many more sys failures than uncorrelated hw faults
	- examples:
		- sw bug that causes every instance of an app server to crash when given a particular bad ip
		- a ***runaway process*** that uses up some shared resource- CPU time, memory, disk space or nw bw
			- ***runaway process***: a process that enters a infinite loop and spawns new processes
		- service that the system depends on that slows down, becomes unresponsive or starts returning corrupted responses
		- cascading failures, failure of one component triggers failure of another
- the bugs that causes the above faults lie ***dormant*** until they are ***triggered*** by an unusual set of circumstances
	- in those circumstances, sw is making some ***assumption*** about it's env - which may have been true, but ***stops being true*** for some reason
- no quick soln for systematic faults in sw, things that can help
	- think about the assumptions and interactions in the sys
	- thorough testing
	- process isolation
	- allowing each process to crash and restart
	- monitoring in prod
	- if a sys provides a guarantee, self-check while it is running and raise an alert if a discrepancy is found

## Human Errors
- config errors by operators was found to be leading cause of outages
- how to make our systems reliable, in spite of unreliable humans?
	- design systems in a way that minimizes opportunities for error
		- API should make it easy to do the right thing
		- should not be too restrictive
	- ***decouples*** places where people make the most mistakes from the places ***where failure is not accepted***
		- ex: provide non-prod sandbox envs
		- sandbox: testing env, purely for experimentation
	- thorough testing
		- unit, whole-system integration tests and manual tests
		- automated testing to cover corner cases
	- allow quick and easy recovery from human errors
		- ex: make it fast to roll back configuration changes, roll out new code gradually, and provide tools to recompute data
	- set up detailed and clear monitoring, such as performance metrics and error rates
		- called telemetry
		- monitoring can show us early warning signals
			- allows us to check whether any assumptions or constraints are being violated

## Scalability
- ability of a system to cope with increased load
- meaningless to say: X is scalable or Y doesn't scale
- discussing scalability means considering questions like:
	- If the system grows in a particular way, what are our options for coping with the growth?
	- how can we add computing resources to handle the additional load?

### Describing load
- describe load first and only then we can discuss growth questions
- load described with a few numbers called ***load parameters***
- best choice of params depend on the arch of our system: 
	- requests per second to web server
	- ratio of reads to writes in db
	- number of active users in the chat room
- twitter use case:
	- post tweet
		- 4.6K req/s on average and 12K/s at peak
	- home timeline
		- 300k requests/s
	- handling 12k writes is easy
	- the problem is fan-out
		- each person's tweet has to be distributed to followers
	- two ways to implement the above ops
		- tweet creation inserts tweet into global coll of tweets
			- when a user requests their TL, look up all the people they follow and find all the tweets and sort them by time
		- maintain a cache for each user's tl
			- when a user posts a tweet, look up all the followers and insert this tweet into each of their caches
			- precomputes TL